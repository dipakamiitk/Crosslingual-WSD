{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_submission.ipynb","provenance":[],"collapsed_sections":["QLE_ddXFD7a0","A9e3_e9sDg-p","UuSLcLXDDpWL","4ds2_wVdM1aO","SoqHh6DSSfvY","ZhyDskKtVZUJ","vRrZpSX3Vepa","zaZAL_RKSnTi"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QLE_ddXFD7a0"},"source":["# 1. Importing Libraries and mounting Google Drive"]},{"cell_type":"code","metadata":{"id":"ylm0zry3C-QQ"},"source":["import os\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfgP7Z_0DgDk","executionInfo":{"status":"ok","timestamp":1607879301688,"user_tz":-330,"elapsed":21361,"user":{"displayName":"nlpbros2 nlp","photoUrl":"","userId":"06536923860707060897"}},"outputId":"439b526a-4e1b-4a63-986e-1b733f601968"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvBFoh9WK4AI","executionInfo":{"status":"ok","timestamp":1607880912200,"user_tz":-330,"elapsed":8924,"user":{"displayName":"nlpbros2 nlp","photoUrl":"","userId":"06536923860707060897"}},"outputId":"c49200d0-605a-42e8-c7d9-7d50b7c96055"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 7.1MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 20.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 38.6MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=69d829c9ad39878989243f9aab1979581a5229ca4b3af34ae1c9625e943d0ef6\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A9e3_e9sDg-p"},"source":["# 2. Load Training and Development Data Paths"]},{"cell_type":"code","metadata":{"id":"MnXb8d1e7spM"},"source":["path = \"/content/drive/My Drive/datasets/submission_datasets/\"\n","### Change this to the "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ri1_mddG9ZU"},"source":["path_training = path + \"train_rev_mcl.csv\"\n","path_mcl_en_rev = path_training\n","ausemcor_supersence_path = path +  \"ausemcor_supersense_pruned.csv\"\n","wic_train_new_format_path =  path + \"wic_train_new_format.csv\"\n","dev_en_rev_path = path + \"dev_en_rev.csv\"\n","devfinal_en_path =  path + \"devfinal-en.csv\"\n","devmix_200_rev_path = path + \"dev_mix_200_rev.csv\"\n","path_train_mcl = path + \"train_mcl.csv\"\n","path_mix6k_train_rev = path + \"mix6k_train_rev\"\n","\n","path_save_3_1 = \"/content/drive/MyDrive/datasets/sub/models/English/xlmr_mcl_rev\"\n","path_save_3_2 = \"/content/drive/MyDrive/datasets/sub/models/English/roberta_signal_mcl_rev_2\"\n","path_save_3_3 = \"/content/drive/MyDrive/datasets/sub/models/English/roberta_signal_aug_rev\"\n","path_save_3_4 = \"/content/drive/MyDrive/datasets/sub/models/English/bert_large_mcl_rev_dev\"\n","\n","path_mix_4_1 = \"/content/drive/MyDrive/datasets/sub/models/Mix/mbert_mix_no_en_1\"\n","path_mix_4_2 = \"/content/drive/MyDrive/datasets/sub/models/Mix/mbert_mix_no_en_no_revmcl_1\"\n","path_mix_4_3 = \"/content/drive/MyDrive/datasets/sub/models/Mix/mbert_mix_no_en_no_revmcl_xlwic_1\"\n","path_mix_4_4 = \"/content/drive/MyDrive/datasets/final/models/Mix/xlmr_mix_no_en_5\"\n","path_mix_4_5 = \"/content/drive/MyDrive/datasets/sub/models/Mix/xlmr_mix_no_en_wic_xlwic\"\n","path_mix_4_6 = \"/content/drive/MyDrive/datasets/sub/models/Mix/xlmr_mix_no_en_rev\"\n","\n","\n","farsi_df_path = path + \"farsi_200_rev.csv\"\n","xl_zh_df_path = path + \"xlwic_zh_val_deepak_format.csv\"\n","xl_nl_df_path = path + \"xlwic_dutch_nl_val_deepak_format.csv\"\n","xl_hr_df_path = path + \"xlwic_croation_hr_val_deepak_format.csv\"\n","xl_da_df_path = path + \"xlwic_danish_da_val_deepak_format.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OoxOLFrQDl09"},"source":["# **3. Training Models - English- English**"]},{"cell_type":"markdown","metadata":{"id":"pfOQO-riZIqU"},"source":["## 3.1 XLM-R MCL-rev"]},{"cell_type":"code","metadata":{"id":"fFHIlMz9ZQjV"},"source":["train_df = pd.read_csv(path_mcl_en_rev)\n","train_df.columns = ['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag','id']\n","train_df = train_df[['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag']]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTCKHegSZQmk"},"source":["dev_df = pd.read_csv(dev_en_rev_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axFDpAXGZQw3"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","\n","\n","from transformers import XLMRobertaTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading roberta tokenizer...')\n","tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n","\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","print('signal added ...')\n","\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","print('Wordpiece indices ....')\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","\n","\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","\n","from transformers import XLMRobertaModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"xlm-roberta-large\"\n","        hidden_states = False\n","        self.encoder = XLMRobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(2048, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","\n","\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","path_this = path_save_3_1\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQe83SxmZQ9q"},"source":["# import random\n","# import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 10\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # if epoch_i ==0:\n","        #   break\n","\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == len(train_dataloader)/2:\n","          print(\" Step Half\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_attention_mask = batch[1].to(device)\n","                  b_poses = batch[2].to(device)\n","                  b_labels = batch[3].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          \n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            flag = True\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_this, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_this, \"roberta_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in train_dataloader:\n","              \n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_attention_mask = batch[1].to(device)\n","                  b_poses = batch[2].to(device)\n","                  b_labels = batch[3].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","          print(\"Train Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(train_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","          \n","          print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Train Inference took: {:}\".format(validation_time))\n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_attention_mask = batch[1].to(device)\n","        b_poses = batch[2].to(device)\n","        b_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_attention_mask = batch[1].to(device)\n","            b_poses = batch[2].to(device)\n","            b_labels = batch[3].to(device)        \n","\n","            last_layer = model_bert(b_input_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    \n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      flag = True\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      torch.save(model_bert.state_dict(), os.path.join(path_this, \"xlmr_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"log_reg_check_best_dev\"))\n","      np.save(os.path.join(path_this, \"best_dev_preds\"), best_val_preds)\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in train_dataloader:\n","        \n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_attention_mask = batch[1].to(device)\n","            b_poses = batch[2].to(device)\n","            b_labels = batch[3].to(device)        \n","\n","            last_layer = model_bert(b_input_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    print(\"Train Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Training Inference took: {:}\".format(validation_time))\n","\n","    # t_labels = []\n","    # t_preds = []\n","\n","    # # Evaluate data for one epoch\n","    # for batch in train_dataloader:\n","        \n","    #     with torch.no_grad():\n","    #         b_input_ids = batch[0].to(device)\n","    #         b_attention_mask = batch[1].to(device)\n","    #         b_poses = batch[2].to(device)\n","    #         b_labels = batch[3].to(device)        \n","\n","    #         last_layer = model_bert(b_input_ids, b_attention_mask)\n","    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n","    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","    #         loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","    #     # Accumulate the validation loss.\n","    #     total_eval_loss += loss.item()\n","\n","    #     # Move logits and labels to CPU\n","    #     logits = logits.cpu().detach().numpy()\n","    #     label_ids = b_labels.cpu().numpy()\n","\n","    #     t_labels.append(label_ids)\n","    #     t_preds.append(logits)\n","        \n","\n","    # all_dev_labels = np.concatenate(t_labels, axis=0)\n","    # all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # # Report the final accuracy for this validation run.\n","    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    # print(\"Train  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # # Calculate the average loss over all of the batches.\n","    # avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # # Measure how long the validation run took.\n","    # validation_time = format_time(time.time() - t0)\n","    \n","    # print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    # print(\"  Training Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EZRlMZFvZS_H"},"source":["## 3.2 Roberta signal en MCL-rev"]},{"cell_type":"code","metadata":{"id":"xpU_wnFRZZ4h"},"source":["train_df = pd.read_csv(path_mcl_en_rev)\n","train_df.columns = ['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag','id']\n","train_df = train_df[['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzDXNPOcZZ8o"},"source":["dev_df = pd.read_csv(dev_en_rev_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_uVoRmOZaAR"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","from transformers import RobertaTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading roberta tokenizer...')\n","tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n","\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","print('Signal Added .. .')\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","\n","print('Wordpiece indices ...')\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","from transformers import RobertaModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"roberta-large\"\n","        hidden_states = False\n","        self.encoder = RobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(2048, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","\n","path_this = path_save_3_2\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmdDnmFdZbN_"},"source":["# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 14\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == len(train_dataloader)/2:\n","          print(\" Step Half\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_attention_mask = batch[1].to(device)\n","                  b_poses = batch[2].to(device)\n","                  b_labels = batch[3].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          actual_val = avg_val_accuracy*20\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_this, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_this, \"roberta_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_attention_mask = batch[1].to(device)\n","        b_poses = batch[2].to(device)\n","        b_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_attention_mask = batch[1].to(device)\n","            b_poses = batch[2].to(device)\n","            b_labels = batch[3].to(device)        \n","\n","            last_layer = model_bert(b_input_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      np.save(os.path.join(path_this, \"dev_preds\"), best_val_preds)\n","      torch.save(model_bert.state_dict(), os.path.join(path_this, \"roberta_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"log_reg_check_best_dev\"))\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n"," \n","    # # Do same on training data\n","    # # Tracking variables \n","    # total_eval_accuracy = 0\n","    # total_eval_loss = 0\n","    # nb_eval_steps = 0\n","\n","    # t_labels = []\n","    # t_preds = []\n","\n","    # # Evaluate data for one epoch\n","    # for batch in train_dataloader:\n","        \n","    #     with torch.no_grad():\n","    #         b_input_ids = batch[0].to(device)\n","    #         b_attention_mask = batch[1].to(device)\n","    #         b_poses = batch[2].to(device)\n","    #         b_labels = batch[3].to(device)        \n","\n","    #         last_layer = model_bert(b_input_ids, b_attention_mask)\n","    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n","    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","    #         loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","    #     # Accumulate the validation loss.\n","    #     total_eval_loss += loss.item()\n","\n","    #     # Move logits and labels to CPU\n","    #     logits = logits.cpu().detach().numpy()\n","    #     label_ids = b_labels.cpu().numpy()\n","\n","    #     t_labels.append(label_ids)\n","    #     t_preds.append(logits)\n","        \n","\n","    # all_dev_labels = np.concatenate(t_labels, axis=0)\n","    # all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # # Report the final accuracy for this validation run.\n","    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    # print(\"Train  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # # Calculate the average loss over all of the batches.\n","    # avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # # Measure how long the validation run took.\n","    # validation_time = format_time(time.time() - t0)\n","    \n","    # print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    # print(\"  Training Validation took: {:}\".format(validation_time))\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YWz80iULZbku"},"source":["## 3.3 Roberta signal en Aug en"]},{"cell_type":"code","metadata":{"id":"6Qh06_KqZgL8"},"source":["train_ausem_df = pd.read_csv(ausemcor_supersence_path).iloc[:,1:]\n","train_ausem_df.columns = ['lemma', 'pos', 'sent1', 'sent2', 'start1', 'end1', 'start2', 'end2', 'tag', 'position1', 'position2' ]\n","\n","train_wic_df = pd.read_csv(wic_train_new_format_path)\n","train_mcl_df = pd.read_csv(path_mcl_en_rev)\n","train_mcl_df.columns = ['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag','id']\n","train_mcl_df = train_mcl_df[['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag']]\n","\n","train_df = pd.concat([train_mcl_df, train_ausem_df, train_wic_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDtzmU2UZgV0"},"source":["dev_df = pd.read_csv(dev_en_rev_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1bIhjynhZgh8"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","from transformers import RobertaTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading roberta tokenizer...')\n","tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","print('Signal Added ....')\n","\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","print('Wordpiece indices ....')\n","# labels = torch.from_numpy(train_gold_df['label'].values)\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 4\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","\n","from transformers import RobertaModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"roberta-large\"\n","        hidden_states = False\n","        self.encoder = RobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(2048, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 5e-6, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","\n","path_this = path_save_3_3\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLEIfAN4Zgpr"},"source":["# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 10\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == len(train_dataloader)/2:\n","          print(\" Step Half\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_attention_mask = batch[1].to(device)\n","                  b_poses = batch[2].to(device)\n","                  b_labels = batch[3].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          actual_val = avg_val_accuracy*20\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_this, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_this, \"roberta_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","          \n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_attention_mask = batch[1].to(device)\n","        b_poses = batch[2].to(device)\n","        b_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_attention_mask = batch[1].to(device)\n","            b_poses = batch[2].to(device)\n","            b_labels = batch[3].to(device)        \n","\n","            last_layer = model_bert(b_input_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      np.save(os.path.join(path_this, \"dev_preds\"), best_val_preds)\n","      torch.save(model_bert.state_dict(), os.path.join(path_this, \"roberta_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"log_reg_check_best_dev\"))\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","    # # Do same on training data\n","    # # Tracking variables \n","    # total_eval_accuracy = 0\n","    # total_eval_loss = 0\n","    # nb_eval_steps = 0\n","\n","    # t_labels = []\n","    # t_preds = []\n","\n","    # # Evaluate data for one epoch\n","    # for batch in train_dataloader:\n","        \n","    #     with torch.no_grad():\n","    #         b_input_ids = batch[0].to(device)\n","    #         b_attention_mask = batch[1].to(device)\n","    #         b_poses = batch[2].to(device)\n","    #         b_labels = batch[3].to(device)        \n","\n","    #         last_layer = model_bert(b_input_ids, b_attention_mask)\n","    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n","    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","    #         loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","    #     # Accumulate the validation loss.\n","    #     total_eval_loss += loss.item()\n","\n","    #     # Move logits and labels to CPU\n","    #     logits = logits.cpu().detach().numpy()\n","    #     label_ids = b_labels.cpu().numpy()\n","\n","    #     t_labels.append(label_ids)\n","    #     t_preds.append(logits)\n","        \n","\n","    # all_dev_labels = np.concatenate(t_labels, axis=0)\n","    # all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # # Report the final accuracy for this validation run.\n","    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    # print(\"Train  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # # Calculate the average loss over all of the batches.\n","    # avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # # Measure how long the validation run took.\n","    # validation_time = format_time(time.time() - t0)\n","    \n","    # print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    # print(\"  Training Validation took: {:}\".format(validation_time))\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTPpAnC9Zg7Y"},"source":["## 3.4 Bert Large Case MCL-rev"]},{"cell_type":"code","metadata":{"id":"aUFmnlirZmfc"},"source":["train_df = pd.read_csv(path_train_mcl)\n","train_df.columns = ['id', 'lemma', 'pos', 'sent1', 'sent2', 'start1', 'end1', 'start2', 'end2', 'tag', 'position1', 'position2']\n","train_df = train_df[['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag']]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntRkDyHhZmoL"},"source":["dev_df = pd.read_csv(dev_en_rev_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWjMqywZZmxg"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","\n","from transformers import BertTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-large-cased')\n","\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","print('Signals Added ...')\n","\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","print(dev_pos_1[:10])\n","print(dev_pos_2[:10])\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","# train_pos_rev_1 = torch.LongTensor(train_pos_rev_1)\n","# train_pos_rev_2 = torch.LongTensor(train_pos_rev_2)\n","\n","# train_pos_rev = torch.stack((train_pos_rev_1, train_pos_rev_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","print('Wordpiece indices. ..')\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'], encoded_inputs_train['token_type_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'], encoded_inputs_dev['token_type_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","from transformers import BertModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"bert-large-cased\"\n","        hidden_states = False\n","        self.encoder = BertModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(2048, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","# Get all of the model's parameters as a list of tuples.\n","params = list(model_bert.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","\n","import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    print(pred_flat)\n","    print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","path_this = path_save_3_4\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dd7F5izQZm6S"},"source":["import random\n","# import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 10\n","best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","      # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_token_type_ids = batch[1].to(device)\n","        b_attention_mask = batch[2].to(device)\n","        b_poses = batch[3].to(device)\n","        b_labels = batch[4].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","          # scheduler.step()\n","        \n","    \n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_token_type_ids = batch[1].to(device)\n","        b_attention_mask = batch[2].to(device)\n","        b_poses = batch[3].to(device)\n","        b_labels = batch[4].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      np.save(os.path.join(path_this, \"dev_preds\"), best_val_preds) \n","      torch.save(model_bert.state_dict(), os.path.join(path_this, \"model_bert_dev_best\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"model_log_reg_dev_best\"))\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","    \n","    # Do same on test data\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in train_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_token_type_ids = batch[1].to(device)\n","        b_attention_mask = batch[2].to(device)\n","        b_poses = batch[3].to(device)\n","        b_labels = batch[4].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():\n","            last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    print(\"Train Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    # Do same on training data\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # # Evaluate data for one epoch\n","    # for batch in train_dataloader:\n","        \n","    #     # Unpack this training batch from our dataloader. \n","    #     #\n","    #     # As we unpack the batch, we'll also copy each tensor to the GPU using \n","    #     # the `to` method.\n","\n","    #     b_input_ids = batch[0].to(device)\n","    #     b_token_type_ids = batch[1].to(device)\n","    #     b_attention_mask = batch[2].to(device)\n","    #     b_poses = batch[3].to(device)\n","    #     b_labels = batch[4].to(device)\n","        \n","    #     # Tell pytorch not to bother with constructing the compute graph during\n","    #     # the forward pass, since this is only needed for backprop (training).\n","    #     with torch.no_grad():\n","    #         last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n","    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\n","    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n","    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","    #         loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","    #     # Accumulate the validation loss.\n","    #     total_eval_loss += loss.item()\n","\n","    #     # Move logits and labels to CPU\n","    #     logits = logits.cpu().detach().numpy()\n","    #     label_ids = b_labels.cpu().numpy()\n","\n","    #     t_labels.append(label_ids)\n","    #     t_preds.append(logits)\n","        \n","\n","    # all_dev_labels = np.concatenate(t_labels, axis=0)\n","    # all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # # Report the final accuracy for this validation run.\n","    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    # print(\"Train  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # # Calculate the average loss over all of the batches.\n","    # avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # # Measure how long the validation run took.\n","    # validation_time = format_time(time.time() - t0)\n","    \n","    # print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    # print(\"  Training Validation took: {:}\".format(validation_time))\n","\n","\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UuSLcLXDDpWL"},"source":["# **4. Training Models - Multilingual**"]},{"cell_type":"markdown","metadata":{"id":"4ds2_wVdM1aO"},"source":["## 4.1  mBERT 8k en non rev + 750 per lang rev\n"]},{"cell_type":"code","metadata":{"id":"NoAGR87x3n5U"},"source":["train_mcl_df = pd.read_csv(path_train_mcl).iloc[:,1:]\r\n","train_mcl_df.columns = ['lemma', 'pos', 'sent1', 'sent2', 'start1', 'end1', 'start2', 'end2', 'tag', 'position1', 'position2']\r\n","mix_df = pd.read_csv(path_mix6k_train_rev).iloc[:,3:]\r\n","mix_df.rename(columns={\"index1\":\"position1\", \"index2\":\"position2\"}, inplace=True)\r\n","train_df = pd.concat([train_mcl_df, mix_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0ofdGXN3jOF"},"source":["dev_df_en = pd.read_csv(devfinal_en_path)\r\n","dev_df_en.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\r\n","dev_df_mix = pd.read_csv(devmix_200_rev_path).iloc[:,1:].rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\r\n","dev_df = pd.concat([dev_df_en, dev_df_mix], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLELb7HrSRcT"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","from transformers import BertTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading bert tokenizer...')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n","\n","\n","# sentences is a list 0f str\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","\n","print(\"Signal Added ...\")\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","print(\"wordpiece indices done ...\")\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'], encoded_inputs_train['token_type_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'], encoded_inputs_dev['token_type_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","from transformers import BertModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"bert-base-multilingual-cased\"\n","        hidden_states = False\n","        self.encoder = BertModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(1536, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat.shape)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","path_mix = path_mix_4_2\n","\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xkt40qB6SRv5"},"source":["# import random\n","# import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 10\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # if epoch_i ==0:\n","        #   break\n","\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == int(len(train_dataloader)/3) or step == int(2*len(train_dataloader)/3):\n","          print(\" Step 1/3rds\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_token_type_ids = batch[1].to(device)\n","                  b_attention_mask = batch[2].to(device)\n","                  b_poses = batch[3].to(device)\n","                  b_labels = batch[4].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","          \n","          \n","          ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","          zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","          fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","          ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","          print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","          print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","          print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","          print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","          print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            flag = True\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_mix, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_mix, \"mbert_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_token_type_ids = batch[1].to(device)\n","        b_attention_mask = batch[2].to(device)\n","        b_poses = batch[3].to(device)\n","        b_labels = batch[4].to(device)        \n","\n","        optimizer.zero_grad()        \n","\n","        last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_token_type_ids = batch[1].to(device)\n","            b_attention_mask = batch[2].to(device)\n","            b_poses = batch[3].to(device)\n","            b_labels = batch[4].to(device)        \n","\n","            last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","\n","\n","    ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","    zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","    fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","    ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","    print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","    print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","    print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","    print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","    print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      torch.save(model_bert.state_dict(), os.path.join(path_mix, \"mbert_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","      np.save(os.path.join(path_mix, \"best_dev_preds\"), best_val_preds)\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SoqHh6DSSfvY"},"source":["## 4.2 mBERT En non rev + farsi + danish + croation + chinese + mcl750 rev 18k points\n"]},{"cell_type":"code","metadata":{"id":"QX7uqwQd4IPp"},"source":["train_mcl_df = pd.read_csv(path_train_mcl).iloc[:,1:]\r\n","train_mcl_df.columns = ['lemma', 'pos', 'sent1', 'sent2', 'start1', 'end1', 'start2', 'end2', 'tag', 'position1', 'position2']\r\n","\r\n","mix_df = pd.read_csv(path_mix6k_train_rev).iloc[:,3:]\r\n","mix_df.rename(columns={\"index1\":\"position1\", \"index2\":\"position2\"}, inplace=True)\r\n","\r\n","farsi_df = pd.read_csv(farsi_df_path).iloc[:,:]\r\n","farsi_df.rename(columns={\"index1\":\"position1\", \"index2\":\"position2\"}, inplace=True)\r\n","xl_zh_df = pd.read_csv(xl_zh_df_path)\r\n","xl_nl_df = pd.read_csv(xl_nl_df_path)\r\n","xl_hr_df = pd.read_csv(xl_hr_df_path)\r\n","xl_da_df = pd.read_csv(xl_da_df_path)\r\n","train_df = pd.concat([train_mcl_df, mix_df, xl_da_df, xl_hr_df, xl_nl_df, xl_zh_df, farsi_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIHRylb04Mw9"},"source":["dev_df_en = pd.read_csv(devfinal_en_path)\r\n","dev_df_en.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\r\n","dev_df_mix = pd.read_csv(devmix_200_rev_path).iloc[:,1:].rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\r\n","dev_df = pd.concat([dev_df_en, dev_df_mix], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hem40D3hSmW3"},"source":["\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","\n","\n","from transformers import BertTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading bert tokenizer...')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n","\n","\n","\n","# sentences is a list 0f str\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","print(\"Signal added ....\")\n","\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","print('wordpiece indices ...')\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'], encoded_inputs_train['token_type_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'], encoded_inputs_dev['token_type_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","from transformers import BertModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"bert-base-multilingual-cased\"\n","        hidden_states = False\n","        self.encoder = BertModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(1536, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat.shape)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","\n","path_mix = path_mix_4_3\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQQ2qYjnSmfs"},"source":["# import random\n","# import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 15\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # if epoch_i ==0:\n","        #   break\n","\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == int(len(train_dataloader)/3) or step == int(2*len(train_dataloader)/3):\n","          print(\" Step 1/3rds\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_token_type_ids = batch[1].to(device)\n","                  b_attention_mask = batch[2].to(device)\n","                  b_poses = batch[3].to(device)\n","                  b_labels = batch[4].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","          \n","          \n","          ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","          zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","          fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","          ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","          print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","          print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","          print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","          print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","          print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            flag = True\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_mix, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_mix, \"mbert_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_token_type_ids = batch[1].to(device)\n","        b_attention_mask = batch[2].to(device)\n","        b_poses = batch[3].to(device)\n","        b_labels = batch[4].to(device)        \n","\n","        optimizer.zero_grad()        \n","\n","        last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_token_type_ids = batch[1].to(device)\n","            b_attention_mask = batch[2].to(device)\n","            b_poses = batch[3].to(device)\n","            b_labels = batch[4].to(device)        \n","\n","            last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","\n","\n","    ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","    zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","    fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","    ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","    print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","    print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","    print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","    print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","    print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      torch.save(model_bert.state_dict(), os.path.join(path_mix, \"mbert_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","      np.save(os.path.join(path_mix, \"best_dev_preds\"), best_val_preds)\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZhyDskKtVZUJ"},"source":["## 4.3 xlmr 8k en rev + 750 per langrev"]},{"cell_type":"code","metadata":{"id":"E9j5-D5JVeGH"},"source":["train_mcl_df = pd.read_csv(path_training)\n","train_mcl_df.columns = ['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag','id']\n","train_mcl_df = train_mcl_df[['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag']]\n","\n","mix_df = pd.read_csv(path_mix6k_train_rev).iloc[:,3:]\n","mix_df.rename(columns={\"index1\":\"position1\", \"index2\":\"position2\"}, inplace=True)\n","\n","train_df = pd.concat([train_mcl_df, mix_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-aGEklVQVePI"},"source":["dev_df_en = pd.read_csv(devfinal_en_path)\n","dev_df_en.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\n","\n","dev_df_mix = pd.read_csv(devmix_200_rev_path).iloc[:,1:].rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\n","dev_df = pd.concat([dev_df_en, dev_df_mix], ignore_index=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRlEWvogQ0QU"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","\n","from transformers import XLMRobertaTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading roberta tokenizer...')\n","tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n","\n","\n","\n","# sentences is a list 0f str\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","\n","print('Signal Added ...')\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","\n","print('wordpiece indexing done....')\n","\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 8\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","from transformers import XLMRobertaModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"xlm-roberta-large\"\n","        hidden_states = False\n","        self.encoder = XLMRobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(2048, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat.shape)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","\n","path_mix = path_mix_4_4\n","\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHsy77rmRGvc"},"source":["# import random\n","# import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 10\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # if epoch_i ==0:\n","        #   break\n","\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == int(len(train_dataloader)/3) or step == int(2*len(train_dataloader)/3):\n","          print(\" Step 1/3rds\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_attention_mask = batch[1].to(device)\n","                  b_poses = batch[2].to(device)\n","                  b_labels = batch[3].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","          \n","          \n","          ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","          zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","          fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","          ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","          print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","          print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","          print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","          print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","          print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            flag = True\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_mix, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_mix, \"roberta_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_attention_mask = batch[1].to(device)\n","        b_poses = batch[2].to(device)\n","        b_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_attention_mask = batch[1].to(device)\n","            b_poses = batch[2].to(device)\n","            b_labels = batch[3].to(device)        \n","\n","            last_layer = model_bert(b_input_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","\n","\n","    ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","    zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","    fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","    ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","    print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","    print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","    print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","    print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","    print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      flag = True\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      torch.save(model_bert.state_dict(), os.path.join(path_mix, \"xlmr_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","      np.save(os.path.join(path_mix, \"best_dev_preds\"), best_val_preds)\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","    \n","\n","    # # Do same on training data\n","    # # Tracking variables \n","    # total_eval_accuracy = 0\n","    # total_eval_loss = 0\n","    # nb_eval_steps = 0\n","\n","    # t_labels = []\n","    # t_preds = []\n","\n","    # # Evaluate data for one epoch\n","    # for batch in train_dataloader:\n","        \n","    #     with torch.no_grad():\n","    #         b_input_ids = batch[0].to(device)\n","    #         b_attention_mask = batch[1].to(device)\n","    #         b_poses = batch[2].to(device)\n","    #         b_labels = batch[3].to(device)        \n","\n","    #         last_layer = model_bert(b_input_ids, b_attention_mask)\n","    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n","    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","    #         loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","    #     # Accumulate the validation loss.\n","    #     total_eval_loss += loss.item()\n","\n","    #     # Move logits and labels to CPU\n","    #     logits = logits.cpu().detach().numpy()\n","    #     label_ids = b_labels.cpu().numpy()\n","\n","    #     t_labels.append(label_ids)\n","    #     t_preds.append(logits)\n","        \n","\n","    # all_dev_labels = np.concatenate(t_labels, axis=0)\n","    # all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # # Report the final accuracy for this validation run.\n","    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    # print(\"Train  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # # Calculate the average loss over all of the batches.\n","    # avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # # Measure how long the validation run took.\n","    # validation_time = format_time(time.time() - t0)\n","    \n","    # print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    # print(\"  Training Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRrZpSX3Vepa"},"source":["## 4.4 xlmr En rev + farsi + danish + croation + chinese + mcl750 rev"]},{"cell_type":"code","metadata":{"id":"F_a75Vg8Via9"},"source":["train_mcl_df = pd.read_csv(path_mcl_en_rev)\n","train_mcl_df.columns = ['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag','id']\n","train_mcl_df = train_mcl_df[['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag']]\n","\n","mix_df = pd.read_csv(path_mix6k_train_rev).iloc[:,3:]\n","mix_df.rename(columns={\"index1\":\"position1\", \"index2\":\"position2\"}, inplace=True)\n","\n","farsi_df = pd.read_csv(farsi_df_path).iloc[:,:]\n","farsi_df.rename(columns={\"index1\":\"position1\", \"index2\":\"position2\"}, inplace=True)\n","xl_zh_df = pd.read_csv(xl_zh_df_path)\n","xl_nl_df = pd.read_csv(xl_nl_df_path)\n","xl_hr_df = pd.read_csv(xl_hr_df_path)\n","xl_da_df = pd.read_csv(xl_da_df_path)\n","\n","train_df = pd.concat([train_mcl_df, mix_df, xl_da_df, xl_hr_df, xl_nl_df, xl_zh_df, farsi_df], ignore_index=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9cEuziPSvm2"},"source":["dev_df_en = pd.read_csv(devfinal_en_path)\n","dev_df_en.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\n","\n","dev_df_mix = pd.read_csv(devmix_200_rev_path).iloc[:,1:].rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\n","dev_df = pd.concat([dev_df_en, dev_df_mix], ignore_index=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjaBfnBLSvwD"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","\n","from transformers import XLMRobertaTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading roberta tokenizer...')\n","tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n","\n","\n","# sentences is a list 0f str\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","\n","\n","print('Signal Added ....')\n","\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","\n","print('Wordpiece indices ...')\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 8\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","from transformers import XLMRobertaModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"xlm-roberta-large\"\n","        hidden_states = False\n","        self.encoder = XLMRobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(2048, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","\n","\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat.shape)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","\n","path_mix = path_mix_4_5\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lGCxf8KVif7"},"source":["# import random\n","# import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 10\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # if epoch_i ==0:\n","        #   break\n","\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == int(len(train_dataloader)/3) or step == int(2*len(train_dataloader)/3):\n","          print(\" Step 1/3rds\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_attention_mask = batch[1].to(device)\n","                  b_poses = batch[2].to(device)\n","                  b_labels = batch[3].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","          \n","          \n","          ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","          zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","          fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","          ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","          print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","          print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","          print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","          print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","          print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            flag = True\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_mix, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_mix, \"roberta_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_attention_mask = batch[1].to(device)\n","        b_poses = batch[2].to(device)\n","        b_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_attention_mask = batch[1].to(device)\n","            b_poses = batch[2].to(device)\n","            b_labels = batch[3].to(device)        \n","\n","            last_layer = model_bert(b_input_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","\n","\n","    ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","    zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","    fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","    ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","    print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","    print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","    print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","    print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","    print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      flag = True\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      torch.save(model_bert.state_dict(), os.path.join(path_mix, \"xlmr_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","      np.save(os.path.join(path_mix, \"best_dev_preds\"), best_val_preds)\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","    \n","\n","    # # Do same on training data\n","    # # Tracking variables \n","    # total_eval_accuracy = 0\n","    # total_eval_loss = 0\n","    # nb_eval_steps = 0\n","\n","    # t_labels = []\n","    # t_preds = []\n","\n","    # # Evaluate data for one epoch\n","    # for batch in train_dataloader:\n","        \n","    #     with torch.no_grad():\n","    #         b_input_ids = batch[0].to(device)\n","    #         b_attention_mask = batch[1].to(device)\n","    #         b_poses = batch[2].to(device)\n","    #         b_labels = batch[3].to(device)        \n","\n","    #         last_layer = model_bert(b_input_ids, b_attention_mask)\n","    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n","    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","    #         loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","    #     # Accumulate the validation loss.\n","    #     total_eval_loss += loss.item()\n","\n","    #     # Move logits and labels to CPU\n","    #     logits = logits.cpu().detach().numpy()\n","    #     label_ids = b_labels.cpu().numpy()\n","\n","    #     t_labels.append(label_ids)\n","    #     t_preds.append(logits)\n","        \n","\n","    # all_dev_labels = np.concatenate(t_labels, axis=0)\n","    # all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # # Report the final accuracy for this validation run.\n","    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    # print(\"Train  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # # Calculate the average loss over all of the batches.\n","    # avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # # Measure how long the validation run took.\n","    # validation_time = format_time(time.time() - t0)\n","    \n","    # print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    # print(\"  Training Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaZAL_RKSnTi"},"source":["## 4.5 xlmr 8k en non rev + 750 per lang rev\n"]},{"cell_type":"code","metadata":{"id":"UMI-RRHhSuAa"},"source":["train_mcl_df = pd.read_csv(path_train_mcl).iloc[:,1:]\n","train_mcl_df.columns = ['lemma', 'pos', 'sent1', 'sent2', 'start1', 'end1', 'start2', 'end2', 'tag', 'position1', 'position2']\n","\n","mix_df = pd.read_csv(path_mix6k_train_rev).iloc[:,3:]\n","mix_df.rename(columns={\"index1\":\"position1\", \"index2\":\"position2\"}, inplace=True)\n","\n","train_df = pd.concat([train_mcl_df, mix_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dxzAI69iSuFV"},"source":["dev_df_en = pd.read_csv(devfinal_en_path)\n","dev_df_en.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\n","\n","dev_df_mix = pd.read_csv(devmix_200_rev_path).iloc[:,1:].rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\n","\n","dev_df = pd.concat([dev_df_en, dev_df_mix], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dauyj1bCSumH"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","\n","\n","from transformers import XLMRobertaTokenizerFast\n","\n","# Load the BERT tokenizer.\n","print('Loading roberta tokenizer...')\n","tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n","\n","\n","# sentences is a list 0f str\n","\n","# Adding a signal without spaces\n","train_sentences_1 =  []\n","train_sentences_2 = []\n","\n","dev_sentences_1 =  []\n","dev_sentences_2 = []\n","\n","\n","\n","list_train_sentences_1 = list(train_df['sent1'])\n","list_train_sentences_2 = list(train_df['sent2'])\n","\n","list_dev_sentences_1 =  list(dev_df['sent1'])\n","list_dev_sentences_2 = list(dev_df['sent2'])\n","\n","\n","for i in range(len(list_train_sentences_1)):\n","  sentence_1 = list_train_sentences_1[i]\n","  # print(sentence_1)\n","  s1 = int(train_df['start1'][i])\n","  e1 = int(train_df['end1'][i])\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n","  train_sentences_1.append(sentence_1)\n","\n","  sentence_2 = list_train_sentences_2[i]\n","  # print(sentence_2)\n","  s2 = int(train_df['start2'][i])\n","  e2 = int(train_df['end2'][i])\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n","  train_sentences_2.append(sentence_2)\n","  # print((train_sentences_1[i], train_sentences_2[i]))\n","\n","for i in range(len(list_dev_sentences_1)):\n","  d_sentence_1 = list_dev_sentences_1[i]\n","  # print(dev_data_df['start1'][i])\n","  s1 = int(dev_df['start1'][i])\n","  e1 = int(dev_df['end1'][i])\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n","  dev_sentences_1.append(d_sentence_1)\n","\n","  d_sentence_2 = list_dev_sentences_2[i]\n","  s2 = int(dev_df['start2'][i])\n","  e2 = int(dev_df['end2'][i])\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n","  dev_sentences_2.append(d_sentence_2)\n","\n","\n","print('Signal Added ....')\n","\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n","\n","# create wordpeice indices of the words of interest now\n","# be aware, that due to signal the actual token have their positions offsetted\n","train_pos_1 = []\n","train_pos_2 = []\n","\n","for j in range(len(train_sentences_1)):\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(train_df['start1'][j])\n","  s2 = int(train_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  train_pos_1.append(pos1)\n","  train_pos_2.append(pos2)\n","# print(train_pos_1)\n","# print(train_pos_2)\n","\n","dev_pos_1 = []\n","dev_pos_2 = []\n","\n","for j in range(len(dev_sentences_1)):\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n","  second_sent = False\n","  s1 = int(dev_df['start1'][j])\n","  s2 = int(dev_df['start2'][j])\n","\n","  for i, offset in enumerate(offsets):\n","    if i == 0:\n","      continue #cls\n","    if offset[1] == 0:\n","      second_sent = True\n","\n","    if offset[0] == s1+1 and second_sent == False:\n","      pos1 = i\n","    elif offset[0] == s2+1 and second_sent == True:\n","      pos2 = i\n","      break\n","  dev_pos_1.append(pos1)\n","  dev_pos_2.append(pos2)\n","\n","\n","train_pos_1 = torch.LongTensor(train_pos_1)\n","train_pos_2 = torch.LongTensor(train_pos_2)\n","\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n","\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\n","\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n","\n","print('Wordpiece Indices ...')\n","\n","\n","train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n","train_labels = torch.from_numpy(train_gold_df_tmp.values)\n","\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n","\n","from torch.utils.data import TensorDataset\n","\n","# Combine the training inputs into a TensorDataset.\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\n","                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n","\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n","\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 8\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","dev_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","from transformers import XLMRobertaModel, AdamW, BertConfig\n","import torch.nn as nn\n","\n","class BERTi(nn.Module):\n","\n","    def __init__(self):\n","        super(BERTi, self).__init__()\n","\n","        options_name = \"xlm-roberta-large\"\n","        hidden_states = False\n","        self.encoder = XLMRobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n","\n","    def forward(self, input_ids, attention_mask):\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\n","\n","        return last_layer\n","\n","\n","class Logistic_Reg(nn.Module):\n","\n","  def __init__(self):\n","    super(Logistic_Reg, self).__init__()\n","\n","    self.fc1 = nn.Linear(2048, 1)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","\n","    return x\n","\n","\n","model_bert = BERTi().to(device)\n","model_log_reg = Logistic_Reg().to(device)\n","optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n","                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n","                  eps = 1e-8 ,\n","                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","def loss_fn(output, targets):\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)\n","\n","\n","def flat_accuracy_single_logit(preds, labels):\n","    pred_flat = (preds>0).flatten()\n","    labels_flat = labels.flatten()\n","    # print(pred_flat.shape)\n","    # print(labels_flat)\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","path_mix = path_mix_4_6\n","\n","\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-cfFi3TSurz"},"source":["# import random\n","# import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","epochs = 10\n","best_val_accuracy = 0\n","t_best_val_accuracy = 0\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model_bert.train()\n","    model_log_reg.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","        # if epoch_i ==0:\n","        #   break\n","\n","        # Progress update every 40 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        if step == int(len(train_dataloader)/3) or step == int(2*len(train_dataloader)/3):\n","          print(\" Step 1/3rds\")\n","          print(\"\")\n","          print(\"Running Validation...\")\n","\n","          t0 = time.time()\n","\n","          # Put the model in evaluation mode--the dropout layers behave differently\n","          # during evaluation.\n","          model_bert.eval()\n","          model_log_reg.eval()\n","\n","          # Tracking variables \n","          total_eval_accuracy = 0\n","          total_eval_loss = 0\n","          nb_eval_steps = 0\n","\n","          t_labels = []\n","          t_preds = []\n","\n","          # Evaluate data for one epoch\n","          for batch in dev_dataloader:\n","\n","              with torch.no_grad():\n","                  b_input_ids = batch[0].to(device)\n","                  b_attention_mask = batch[1].to(device)\n","                  b_poses = batch[2].to(device)\n","                  b_labels = batch[3].to(device)        \n","\n","                  last_layer = model_bert(b_input_ids, b_attention_mask)\n","                  b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","                  gathered_activations = torch.gather(last_layer, 1, b_poses)\n","                  logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","                  loss = loss_fn(logits, b_labels.type_as(logits))\n","                  \n","              # Accumulate the validation loss.\n","              total_eval_loss += loss.item()\n","\n","              # Move logits and labels to CPU\n","              logits = logits.cpu().detach().numpy()\n","              # print(logits)\n","              label_ids = b_labels.cpu().numpy()\n","\n","              t_labels.append(label_ids)\n","              t_preds.append(logits)\n","              \n","\n","          all_dev_labels = np.concatenate(t_labels, axis=0)\n","          all_dev_logits = np.concatenate(t_preds, axis=0)\n","          # Report the final accuracy for this validation run.\n","          # print(all_dev_logits)\n","          avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","          print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","          en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","          \n","          \n","          ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","          zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","          fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","          ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","          print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","          print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","          print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","          print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","          print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","\n","\n","          # Calculate the average loss over all of the batches.\n","          avg_val_loss = total_eval_loss / len(dev_dataloader)\n","          \n","          # Measure how long the validation run took.\n","          validation_time = format_time(time.time() - t0)\n","\n","          if(avg_val_accuracy > best_val_accuracy):\n","            flag = True\n","            best_val_accuracy = avg_val_accuracy\n","            best_val_preds = all_dev_logits\n","            np.save(os.path.join(path_mix, \"dev_preds\"), best_val_preds)\n","            torch.save(model_bert.state_dict(), os.path.join(path_mix, \"roberta_check_best_dev\"))\n","            torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","          \n","          print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","          print(\"  Validation took: {:}\".format(validation_time))\n","\n","          model_bert.train()\n","          model_log_reg.train()\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","\n","        b_input_ids = batch[0].to(device)\n","        b_attention_mask = batch[1].to(device)\n","        b_poses = batch[2].to(device)\n","        b_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        optimizer.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\n","\n","        # print(b_poses[:,0].size())\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","        # print(b_poses.size())\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\n","\n","        # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n","\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","        # print(type(b_labels))\n","        loss = loss_fn(logits, b_labels.type_as(logits))\n","        # print(\"loss = \"+str(loss))\n","        # print(logits.cpu().detach().numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        \n","        # print(loss)\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        # TODO logsistic REG????/\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # # Update the learning rate.\n","        # scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    \n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model_bert.eval()\n","    model_log_reg.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    t_labels = []\n","    t_preds = []\n","\n","    # Evaluate data for one epoch\n","    for batch in dev_dataloader:\n","\n","        with torch.no_grad():\n","            b_input_ids = batch[0].to(device)\n","            b_attention_mask = batch[1].to(device)\n","            b_poses = batch[2].to(device)\n","            b_labels = batch[3].to(device)        \n","\n","            last_layer = model_bert(b_input_ids, b_attention_mask)\n","            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","            gathered_activations = torch.gather(last_layer, 1, b_poses)\n","            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","            loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.cpu().detach().numpy()\n","        # print(logits)\n","        label_ids = b_labels.cpu().numpy()\n","\n","        t_labels.append(label_ids)\n","        t_preds.append(logits)\n","        \n","\n","    all_dev_labels = np.concatenate(t_labels, axis=0)\n","    all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # Report the final accuracy for this validation run.\n","    # print(all_dev_logits)\n","    avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits[200:], all_dev_labels[200:])\n","    print(\"Validation Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","    en_dev_acc = flat_accuracy_single_logit(all_dev_logits[:200], all_dev_labels[:200])\n","\n","\n","    ar_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,:50].ravel(), all_dev_labels[200:].reshape(2,200)[:,:50].ravel())\n","    zh_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,50:100].ravel(), all_dev_labels[200:].reshape(2,200)[:,50:100].ravel())\n","    fr_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,100:150].ravel(), all_dev_labels[200:].reshape(2,200)[:,100:150].ravel())\n","    ru_dev_acc = flat_accuracy_single_logit(all_dev_logits[200:].reshape(2,200)[:,150:200].ravel(), all_dev_labels[200:].reshape(2,200)[:,150:200].ravel())\n","    print(\"EN dev Accuracy: {0:.4f}\".format(en_dev_acc), end = \",\")\n","    print(\"RU dev Accuracy: {0:.4f} \".format(ru_dev_acc), end = \",\")\n","    print(\"ZH dev Accuracy: {0:.4f} \".format(zh_dev_acc), end = \",\")\n","    print(\"AR dev Accuracy: {0:.4f} \".format(ar_dev_acc), end = \",\")\n","    print(\"FR dev Accuracy: {0:.4f} \".format(fr_dev_acc))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(dev_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","\n","    if(avg_val_accuracy > best_val_accuracy):\n","      flag = True\n","      best_val_accuracy = avg_val_accuracy\n","      best_val_preds = all_dev_logits\n","      torch.save(model_bert.state_dict(), os.path.join(path_mix, \"xlmr_check_best_dev\"))\n","      torch.save(model_log_reg.state_dict(), os.path.join(path_mix, \"log_reg_check_best_dev\"))\n","      np.save(os.path.join(path_mix, \"best_dev_preds\"), best_val_preds)\n","    \n","    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","    \n","\n","    # # Do same on training data\n","    # # Tracking variables \n","    # total_eval_accuracy = 0\n","    # total_eval_loss = 0\n","    # nb_eval_steps = 0\n","\n","    # t_labels = []\n","    # t_preds = []\n","\n","    # # Evaluate data for one epoch\n","    # for batch in train_dataloader:\n","        \n","    #     with torch.no_grad():\n","    #         b_input_ids = batch[0].to(device)\n","    #         b_attention_mask = batch[1].to(device)\n","    #         b_poses = batch[2].to(device)\n","    #         b_labels = batch[3].to(device)        \n","\n","    #         last_layer = model_bert(b_input_ids, b_attention_mask)\n","    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n","    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n","    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n","    #         loss = loss_fn(logits, b_labels.type_as(logits))\n","            \n","    #     # Accumulate the validation loss.\n","    #     total_eval_loss += loss.item()\n","\n","    #     # Move logits and labels to CPU\n","    #     logits = logits.cpu().detach().numpy()\n","    #     label_ids = b_labels.cpu().numpy()\n","\n","    #     t_labels.append(label_ids)\n","    #     t_preds.append(logits)\n","        \n","\n","    # all_dev_labels = np.concatenate(t_labels, axis=0)\n","    # all_dev_logits = np.concatenate(t_preds, axis=0)\n","    # # Report the final accuracy for this validation run.\n","    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n","    # print(\"Train  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","    # # Calculate the average loss over all of the batches.\n","    # avg_val_loss = total_eval_loss / len(train_dataloader)\n","    \n","    # # Measure how long the validation run took.\n","    # validation_time = format_time(time.time() - t0)\n","    \n","    # print(\"  Train Avg Loss: {0:.3f}\".format(avg_val_loss))\n","    # print(\"  Training Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sut1pNKTDssZ"},"source":["# 5. **Predicting labels for given test set**"]},{"cell_type":"markdown","metadata":{"id":"wT0RVY0sDHyh"},"source":["## 5.1 MultiLingual Inference Codes"]},{"cell_type":"code","metadata":{"id":"zZ3hQTb_TVlR"},"source":["# folder paths of saved models\r\n","path_mbert_1_1 = \"/content/drive/MyDrive/datasets/sub/models/Mix/mbert_mix_no_en_no_revmcl_1/\"\r\n","path_mbert_1_2 = \"/content/drive/MyDrive/datasets/sub/models/Mix/mbert_mix_no_en_no_revmcl_xlwic_1/\"\r\n","\r\n","path_xlmr_1_1 = \"/content/drive/MyDrive/datasets/sub/models/Mix/xlmr_mix_no_en_rev/\"\r\n","path_xlmr_1_2 = \"/content/drive/MyDrive/datasets/sub/models/Mix/xlmr_mix_no_en_wic_xlwic/\"\r\n","path_xlmr_1_3 = \"/content/drive/MyDrive/datasets/final/models/Mix/xlmr_mix_no_en_5/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGt6tPojDpAn"},"source":["import os\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c-WmARhDpAo","executionInfo":{"status":"ok","timestamp":1607949902271,"user_tz":-330,"elapsed":19787,"user":{"displayName":"NLP Bros","photoUrl":"","userId":"04584571589891323382"}},"outputId":"8e8e1369-3be3-44b5-c0af-104a7ad2fedd"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d-KbSYQhDpAq"},"source":["test_df = pd.read_csv(\"/content/drive/MyDrive/datasets/final/datasets/Non-English/nonenglishtest.csv\")\r\n","test_df.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\r\n","print(test_df['id'])\r\n","test_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLkuY-ooScxd"},"source":["dev_df = pd.read_csv(\"/content/drive/MyDrive/datasets/final/datasets/Mix/dev_mix_200_rev.csv\")\r\n","dev_df.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\r\n","tags = dev_df['tag'].to_numpy()\r\n","print(dev_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6Tgc2MADpAs","executionInfo":{"status":"ok","timestamp":1607956770488,"user_tz":-330,"elapsed":1116,"user":{"displayName":"NLP Bros","photoUrl":"","userId":"04584571589891323382"}},"outputId":"fac5a3b2-05a0-4282-8e75-1ad3f7914514"},"source":["import torch\r\n","\r\n","# If there's a GPU available...\r\n","if torch.cuda.is_available():    \r\n","\r\n","    # Tell PyTorch to use the GPU.    \r\n","    device = torch.device(\"cuda\")\r\n","\r\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n","\r\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n","\r\n","# If not...\r\n","else:\r\n","    print('No GPU available, using the CPU instead.')\r\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ej5MXy1DDpAs","executionInfo":{"status":"ok","timestamp":1607956776976,"user_tz":-330,"elapsed":6851,"user":{"displayName":"NLP Bros","photoUrl":"","userId":"04584571589891323382"}},"outputId":"5c7d7667-543e-4be3-c853-f6cd6f82c3e9"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujqWpCYIDpAt","executionInfo":{"status":"ok","timestamp":1607956776977,"user_tz":-330,"elapsed":6003,"user":{"displayName":"NLP Bros","photoUrl":"","userId":"04584571589891323382"}},"outputId":"58f0796f-deb0-4bc7-ef7f-3dffd2f4b509"},"source":["# Adding a signal without spaces\r\n","test_sentences_1 =  []\r\n","test_sentences_2 = []\r\n","\r\n","dev_sentences_1 =  []\r\n","dev_sentences_2 = []\r\n","\r\n","list_test_sentences_1 = list(test_df['sent1'])\r\n","list_test_sentences_2 = list(test_df['sent2'])\r\n","\r\n","list_dev_sentences_1 =  list(dev_df['sent1'])\r\n","list_dev_sentences_2 = list(dev_df['sent2'])\r\n","\r\n","for i in range(len(list_test_sentences_1)):\r\n","  sentence_1 = list_test_sentences_1[i]\r\n","  # print(sentence_1)\r\n","  s1 = int(test_df['start1'][i])\r\n","  e1 = int(test_df['end1'][i])\r\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\r\n","  test_sentences_1.append(sentence_1)\r\n","\r\n","  sentence_2 = list_test_sentences_2[i]\r\n","  # print(sentence_2)\r\n","  s2 = int(test_df['start2'][i])\r\n","  e2 = int(test_df['end2'][i])\r\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\r\n","  test_sentences_2.append(sentence_2)\r\n","  # print((test_sentences_1[i], test_sentences_2[i]))\r\n","\r\n","for i in range(len(list_dev_sentences_1)):\r\n","  d_sentence_1 = list_dev_sentences_1[i]\r\n","  # print(dev_data_df['start1'][i])\r\n","  s1 = int(dev_df['start1'][i])\r\n","  e1 = int(dev_df['end1'][i])\r\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\r\n","  dev_sentences_1.append(d_sentence_1)\r\n","\r\n","  d_sentence_2 = list_dev_sentences_2[i]\r\n","  s2 = int(dev_df['start2'][i])\r\n","  e2 = int(dev_df['end2'][i])\r\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\r\n","  dev_sentences_2.append(d_sentence_2)\r\n","\r\n","\r\n","print((test_sentences_2[:10]))\r\n","print((test_sentences_1[:10]))\r\n","print(dev_sentences_1[:10])\r\n","print(dev_sentences_2[:10])\r\n","\r\n","\r\n","dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\r\n","dev_labels = torch.from_numpy(dev_gold_df_tmp.values)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Утверждение о том, что другие тюрьмы \"находятся\" в аналогичном состоянии, можно считать достаточно убедительным.', 'Поэтому мы поддержали тот \"путь\", который был предложен Председателем, и полностью приняли видоизмененную повестку дня 1996 года в качестве \"наименьшего общего знаменателя\".', 'Развитие представляет собой многоаспектную задачу по \"повышению\" качества жизни всех людей.', 'Совершенно очевидно, что следствием этого может стать ухудшение \"качества\" услуг, оказываемых БАПОР, а в конечном счете - ухудшение условий жизни беженцев.', 'В ходе подготовки к специальной сессии была также \"выпущена\" в эфир специальная радиопрограмма в четырех частях об Организации Объединенных Наций на других языках.', 'Как \"показано\" в приложении III, балансовая стоимость такого проданного имущества составляла 791 400 долл. США, а стоимость реализации, соответственно, 632 900 долл. США.', 'По просьбе Группы Организации Объединенных Наций по оказанию помощи в проведении выборов в Южной Африке и Мозамбике ПГД организовала делегации из числа членов рабочей группы по Африке и работала вместе с Департаментом по гуманитарным вопросам Секретариата Организации Объединенных Наций над организацией совместного с ДГВ практикума по раннему \"предупреждению\" для мобилизации учреждений Организации Объединенных Наций и других участников на действия по раннему предупреждению и предотвращению неблагоприятного развития событий.', 'Международное сообщество должно сделать все для \"предупреждения\" конфликтов, направляя свои усилия на устранение их социально-экономических причин.', 'C признательностью отмечается \"вклад\" в развитие некоторых территорий, вносимый специализированными учреждениями и другими организациями системы Организации Объединенных Наций.', 'Во-первых, необходимо дать договорным органам четкие \"указания\" относительно того, каким образом следует расходовать конкретные бюджетные средства, предназначенные для целей информирования общественности.']\n","['На территории Замбии \"находится\" большое число лиц из Западной Африки, которые осели, интегрировались в замбийское общество и преуспевают.', 'Это был нелегкий \"путь\", но пока что в процессе следования по нему Республике Беларусь удалось, однако, избежать крупных обвалов в экономике и политике, и при адекватной помощи международного сообщества мы надеемся достичь определенных стандартов, сопоставимых с уровнем развитых стран мира.', '\"Повышение\" эффективности работы таможенной администрации способствовало бы проведению более эффективной таможенной обработки внешнеторговых операций в минимальные сроки, что привело бы к снижению расходов, связанных с хранением товаров.', 'Поскольку во многих международных документах запрещается использовать голод и болезни в \"качестве\" средств оказания политического давления, неизбежно встает вопрос о том, все ли страны имеют равные права.', 'В 11 ч. 00 м. произраильское ополчение \"выпустило\" минометную мину по окрестностям Брашита с территории оккупированного сектора.', 'Оценка этих результатов с использованием метода анализа изменения затрат \"показала\" весьма устойчивый характер деятельности Базы.', 'Таким образом, Отделение по \"предупреждению\" преступности и уголовному правосудию Секретариата Организации Объединенных Наций получило возможность использовать вклад в разработку выбранной проблематики международного научного сообщества.', 'Таким образом, Отделение по \"предупреждению\" преступности и уголовному правосудию Секретариата Организации Объединенных Наций получило возможность использовать вклад в разработку выбранной проблематики международного научного сообщества.', 'В меру своих возможностей она вносит значительный \"вклад\" в поддержание мира и смягчение последствий конфликтов.', 'Было высказано мнение, что \"указание\" в пункте 53 на запрос предложений в отношении пересмотренных спецификаций и условий контракта может означать, что условия контракта открыты для переговоров даже до принятия окончательного решения о выдаче подряда.']\n","['وذكروا أنهم \"فُصلوا\" عن أسرهم واحتُجزوا مؤقتاً في مواقع مختلفة إلى أن نُقلوا إلى سجن عادي.', 'قد تكون \"رائحة\" الفم سبب في انفصال المخطوبين أو المتزوجين، أو سبب في خسارة العمل لذا يجب الاهتمام بنظافة الفم .', 'واستخدمت \"قوة\" اﻷمم المتحدة المؤقتة في لبنان قنواتها الخاصة للحصول على معلومات عن حالة الموظف وطلبت زيارته، وطلب أيضا المستشار القانوني مساعدة وزارة الخارجية في إطلاق سراح الموظف، طالبا إياها تقديم اﻷسباب الداعية للقبض عليه والسماح لقوة اﻷمم المتحدة المؤقتة في لبنان بزيارته.', 'كما كتب القصة القصيرة جدا سماها قصة تقرا في دقيقة واحدة نشر أغلبها في \"ملحق\" جريدة العمل الثقافي بشكل أسبوعي.', 'وهكذا نشبت في القليعة \"معركة\" عنيفة بين المرابطين والأرجونيين، ويضع ابن القطان تاريخها في سنة 523 هـ (1129)، ويقول لنا إن قوات المسلمين كلها كانت بقيادة ابن مجور، وأن المسلمين أصيبوا فيها بهزيمة فادحة، وفنى معظمهم قتلا وأسراً.', 'إن ما تقدم يعبر عن اﻹرادة الجماعية للمجتمع الدولي الذي \"خص\" عملية السلام في الشرق اﻷوسط بأعلى درجات اﻷولوية والذي يحدوه اﻷمل في إتمام التسوية العربية اﻹسرائيلية قبل نهاية هذا القرن.', 'ويدعي أصحاب مطالبات تتعلق بالنقل البحري تكبد خسائر بخصوص عمليات نقل إلى مواني تقع في الجزء الجنوبي \"للخليج\" الفارسي وقناة السويس والبحر الأحمر.', 'ويجري العمل أيضا على إعداد \"دليل\" المنظمات غير الحكومية في الضفة الغربية، من المقرر نشره أيضا في عام ١٩٩٨.', 'يتم بعد ذلك تقييم \"الحملة\" عسكرية على أساس مدي تحقيق الغايات والأهداف المخطط لها من خلال القتالية وغير القتالية.', 'كما أنه برهن على ما يمثله من خطر متأصل فيه ومستمر على النظام العام بما قام به من محاولتين عنيفتين \"للهروب\" من السجن، وذلك في أيلول/سبتمبر 1995 وتموز/يوليه 1997، عوقب على كل منهما بالسجن لفترة ثمانية أشهر.']\n","['والمسافة التي \"تفصل\" المستوطنين عن حقولهم لا تتجاوز 500 متر، مما ييسر وصولهم إليها.', 'في غضون خمس دقائق يتعلمون الربط بين \"الرائحة\" وتوفير المواد الغذائية، وهذا يسبب في تمدد الخرطوم كرد فعل.', 'فعلى الرغم من أن البلدان الناشئة في مجال الفضاء أمم محدودة الموارد، فهي تشكل مجتمعة سوقا كبيرة و\"قوة\" دافعة ﻻستغلال الفضاء.', 'عادة، يعمل \"الملحق\" العسكري في السلك الدبلوماسي للسفارة أو القنصلية مع الاحتفاظ بلجنة عسكرية.', 'شوهدت حافلات \"المعركة\" الانتخابية لأول مرة في أواخر سبعينيات القرن العشرين.', 'وينبغي اﻻستمرار في توجيه المساعي بحيث تلبي رغبات شعوب اﻷقاليم المتبقية غير المتمتعة بالحكم الذاتي، فاختيار نوع النظام السياسي المفضل هو أمر \"يخص\" شعوب تلك اﻷقاليم.', 'فمعظم اﻻكتشافات الجديدة الرئيسية تمت في المياه العميقة، وآخرها قبالة سواحل غرب أفريقيا وفي \"خليج\" المكسيك.', 'غير أنه ﻻ يوجد \"دليل\" يشير إلى استخدام الحرق أو طريقة معالجة مماثلة في هذه الحالة.', 'أسفرت المعركة عن هزيمة الصليبيين هزيمة كبرى منعتهم من إرسال \"حملة\" صليبية جديدة إلى مصر، وكانت بمثابة نقطة البداية التي أخذت بعدها الهزائم تتوالى عليهم حتى تم تحرير كامل الشام من الحكم الصليبي.', 'ولا شك في أن هذا السجين أبدى مهارة كبيرة في \"الهروب\" من السجن، إلا أن من الواضح أنه لا بد من إيجاد سبل أخرى لمنعه من الهروب دون الإخلال، كما يجري حالياً، بالقواعد النموذجية الدنيا لمعاملة السجناء، والأحكام ذات الصلة التي اعتمدها المجلس الاقتصادي والاجتماعي والتي اعتمدتها الجمعية العامة للأمم المتحدة.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h2iGDfllDpAt"},"source":["def loss_fn(output, targets):\r\n","  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SVR3fFTDpAt"},"source":["def flat_accuracy_single_logit(preds, labels):\r\n","    pred_flat = (preds>0).flatten()\r\n","    labels_flat = labels.flatten()\r\n","    # print(pred_flat)\r\n","    # print(labels_flat)\r\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lkXTGZADpAt"},"source":["import time\r\n","import datetime\r\n","\r\n","def format_time(elapsed):\r\n","    '''\r\n","    Takes a time in seconds and returns a string hh:mm:ss\r\n","    '''\r\n","    # Round to the nearest second.\r\n","    elapsed_rounded = int(round((elapsed)))\r\n","    \r\n","    # Format as hh:mm:ss\r\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZOGJculDPWG"},"source":["**mBERT Preprocessing**"]},{"cell_type":"code","metadata":{"id":"U_U_PrnoDN84"},"source":["from transformers import BertTokenizerFast\r\n","\r\n","# Load the BERT tokenizer.\r\n","print('Loading bert tokenizer...')\r\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\r\n","\r\n","encoded_inputs_test = tokenizer(test_sentences_1, test_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","\r\n","\r\n","# create wordpeice indices of the words of interest now\r\n","# be aware, that due to signal the actual token have their positions offsetted\r\n","test_pos_1 = []\r\n","test_pos_2 = []\r\n","\r\n","for j in range(len(test_sentences_1)):\r\n","  offsets = encoded_inputs_test['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(test_df['start1'][j])\r\n","  s2 = int(test_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  test_pos_1.append(pos1)\r\n","  test_pos_2.append(pos2)\r\n","# print(train_pos_1)\r\n","# print(train_pos_2)\r\n","\r\n","dev_pos_1 = []\r\n","dev_pos_2 = []\r\n","\r\n","for j in range(len(dev_sentences_1)):\r\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(dev_df['start1'][j])\r\n","  s2 = int(dev_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  dev_pos_1.append(pos1)\r\n","  dev_pos_2.append(pos2)\r\n","\r\n","test_pos_1 = torch.LongTensor(test_pos_1)\r\n","test_pos_2 = torch.LongTensor(test_pos_2)\r\n","\r\n","test_pos = torch.stack((test_pos_1, test_pos_2), dim =1)\r\n","\r\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\r\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\r\n","\r\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\r\n","\r\n","from torch.utils.data import TensorDataset\r\n","\r\n","test_dataset = TensorDataset(encoded_inputs_test['input_ids'], encoded_inputs_test['token_type_ids'],\r\n","                              encoded_inputs_test['attention_mask'], test_pos)\r\n","\r\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'], encoded_inputs_dev['token_type_ids'],\r\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\r\n","\r\n","batch_size = 16\r\n","\r\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","# Create the DataLoaders for our training and validation sets.\r\n","# We'll take training samples in random order. \r\n","test_dataloader = DataLoader(\r\n","            test_dataset,  # The training samples.\r\n","            sampler = SequentialSampler(test_dataset), # Select batches randomly\r\n","            batch_size = batch_size # Trains with this batch size.\r\n","        )\r\n","\r\n","# For validation the order doesn't matter, so we'll just read them sequentially.\r\n","dev_dataloader = DataLoader(\r\n","            dev_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\r\n","            batch_size = batch_size # Evaluate with this batch size.\r\n","        )\r\n","\r\n","from transformers import BertModel, AdamW, BertConfig\r\n","import torch.nn as nn\r\n","\r\n","class BERTi(nn.Module):\r\n","\r\n","    def __init__(self):\r\n","        super(BERTi, self).__init__()\r\n","\r\n","        options_name = \"bert-base-multilingual-cased\"\r\n","        hidden_states = False\r\n","        self.encoder = BertModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\r\n","\r\n","    def forward(self, input_ids, token_type_ids, attention_mask):\r\n","        last_layer, _ = self.encoder(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\r\n","\r\n","        return last_layer\r\n","\r\n","class Logistic_Reg(nn.Module):\r\n","\r\n","  def __init__(self):\r\n","    super(Logistic_Reg, self).__init__()\r\n","\r\n","    self.fc1 = nn.Linear(1536, 1)\r\n","\r\n","  def forward(self, x):\r\n","    x = self.fc1(x)\r\n","\r\n","    return x\r\n","\r\n","model_bert = BERTi().to(device)\r\n","model_log_reg = Logistic_Reg().to(device)\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5cg2yPtXNEJ"},"source":["**mBERT 1.1**"]},{"cell_type":"code","metadata":{"id":"NdkdYYwwUNDv"},"source":["model_bert.load_state_dict(torch.load(os.path.join(path_mbert_1_1, 'mbert_check_best_dev')))\r\n","model_log_reg.load_state_dict(torch.load(os.path.join(path_mbert_1_1, 'log_reg_check_best_dev')))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in test_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_token_type_ids = batch[1].to(device)\r\n","        b_attention_mask = batch[2].to(device)\r\n","        b_poses = batch[3].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","mbert_en_no_rev_test = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_token_type_ids = batch[1].to(device)\r\n","        b_attention_mask = batch[2].to(device)\r\n","        b_poses = batch[3].to(device)\r\n","        b_labels = batch[4].to(device)        \r\n","\r\n","        last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        loss = loss_fn(logits, b_labels.type_as(logits))\r\n","        \r\n","    # Accumulate the validation loss.\r\n","    total_eval_loss += loss.item()\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","    label_ids = b_labels.cpu().numpy()\r\n","\r\n","    t_labels.append(label_ids)\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_labels = np.concatenate(t_labels, axis=0)\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","# Report the final accuracy for this validation run.\r\n","# print(all_dev_logits)\r\n","avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\r\n","          \r\n","print(\"Dev Accuracy: {0:.4f}\".format(avg_val_accuracy))\r\n","\r\n","mbert_en_no_rev_dev = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kA2euIUd2oDy"},"source":["**mBERT 1.2**"]},{"cell_type":"code","metadata":{"id":"FC7YF4ThYET4"},"source":["model_bert.load_state_dict(torch.load(os.path.join(path_mbert_1_2, 'mbert_check_best_dev')))\r\n","model_log_reg.load_state_dict(torch.load(os.path.join(path_mbert_1_2, 'log_reg_check_best_dev')))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in test_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_token_type_ids = batch[1].to(device)\r\n","        b_attention_mask = batch[2].to(device)\r\n","        b_poses = batch[3].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","mbert_xlwic_test = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_token_type_ids = batch[1].to(device)\r\n","        b_attention_mask = batch[2].to(device)\r\n","        b_poses = batch[3].to(device)\r\n","        b_labels = batch[4].to(device)        \r\n","\r\n","        last_layer = model_bert(b_input_ids,b_token_type_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 768)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        loss = loss_fn(logits, b_labels.type_as(logits))\r\n","        \r\n","    # Accumulate the validation loss.\r\n","    total_eval_loss += loss.item()\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","    label_ids = b_labels.cpu().numpy()\r\n","\r\n","    t_labels.append(label_ids)\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_labels = np.concatenate(t_labels, axis=0)\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","# Report the final accuracy for this validation run.\r\n","# print(all_dev_logits)\r\n","avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\r\n","          \r\n","print(\"Dev Accuracy: {0:.4f}\".format(avg_val_accuracy))\r\n","\r\n","mbert_xlwic_dev = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYEHTTE6Yr6l"},"source":["**XLMR 1.1 with XLMR Preprocessing**"]},{"cell_type":"code","metadata":{"id":"4iT69tn_YeTx"},"source":["#xlmr_mcl\r\n","from transformers import XLMRobertaTokenizerFast\r\n","\r\n","# Load the BERT tokenizer.\r\n","print('Loading roberta tokenizer...')\r\n","tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\r\n","\r\n","encoded_inputs_test = tokenizer(test_sentences_1, test_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_test['input_ids'][0]))\r\n","print(type(encoded_inputs_test['input_ids']))\r\n","\r\n","\r\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_dev['input_ids'][0]))\r\n","print(type(encoded_inputs_dev['input_ids']))\r\n","\r\n","# create wordpeice indices of the words of interest now\r\n","# be aware, that due to signal the actual token have their positions offsetted\r\n","test_pos_1 = []\r\n","test_pos_2 = []\r\n","\r\n","for j in range(len(test_sentences_1)):\r\n","  offsets = encoded_inputs_test['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(test_df['start1'][j])\r\n","  s2 = int(test_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  test_pos_1.append(pos1)\r\n","  test_pos_2.append(pos2)\r\n","# print(train_pos_1)\r\n","# print(test_pos_2)\r\n","\r\n","dev_pos_1 = []\r\n","dev_pos_2 = []\r\n","\r\n","for j in range(len(dev_sentences_1)):\r\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(dev_df['start1'][j])\r\n","  s2 = int(dev_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  dev_pos_1.append(pos1)\r\n","  dev_pos_2.append(pos2)\r\n","\r\n","test_pos_1 = torch.LongTensor(test_pos_1)\r\n","test_pos_2 = torch.LongTensor(test_pos_2)\r\n","\r\n","test_pos = torch.stack((test_pos_1, test_pos_2), dim =1)\r\n","\r\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\r\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\r\n","\r\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\r\n","\r\n","from torch.utils.data import TensorDataset\r\n","\r\n","# Combine the testing inputs into a TensorDataset.\r\n","test_dataset = TensorDataset(encoded_inputs_test['input_ids'],\r\n","                              encoded_inputs_test['attention_mask'], test_pos)\r\n","\r\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\r\n","                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\r\n","\r\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","# The DataLoader needs to know our batch size for testing, so we specify it \r\n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n","# size of 16 or 32.\r\n","batch_size = 8\r\n","\r\n","# Create the DataLoaders for our training and validation sets.\r\n","# We'll take testing samples in random order. \r\n","test_dataloader = DataLoader(\r\n","            test_dataset,  # The training samples.\r\n","            sampler = SequentialSampler(test_dataset), # Select batches randomly\r\n","            batch_size = batch_size # Trains with this batch size.\r\n","        )\r\n","\r\n","# For validation the order doesn't matter, so we'll just read them sequentially.\r\n","dev_dataloader = DataLoader(\r\n","            dev_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\r\n","            batch_size = batch_size # Evaluate with this batch size.\r\n","        )\r\n","\r\n","from transformers import XLMRobertaModel, AdamW, BertConfig\r\n","import torch.nn as nn\r\n","\r\n","class BERTi(nn.Module):\r\n","\r\n","    def __init__(self):\r\n","        super(BERTi, self).__init__()\r\n","\r\n","        options_name = \"xlm-roberta-large\"\r\n","        hidden_states = False\r\n","        self.encoder = XLMRobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\r\n","\r\n","    def forward(self, input_ids, attention_mask):\r\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\r\n","\r\n","        return last_layer\r\n","\r\n","class Logistic_Reg(nn.Module):\r\n","\r\n","  def __init__(self):\r\n","    super(Logistic_Reg, self).__init__()\r\n","\r\n","    self.fc1 = nn.Linear(2048, 1)\r\n","\r\n","  def forward(self, x):\r\n","    x = self.fc1(x)\r\n","\r\n","    return x\r\n","\r\n","model_bert = BERTi().to(device)\r\n","model_log_reg = Logistic_Reg().to(device)\r\n","\r\n","\r\n","\r\n","model_bert.load_state_dict(torch.load(os.path.join(path_xlmr_1_1,'roberta_check_best_dev')))\r\n","model_log_reg.load_state_dict(torch.load(os.path.join(path_xlmr_1_1, 'log_reg_check_best_dev')))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in test_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","xlmr_87_test = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","        b_labels = batch[3].to(device)        \r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        loss = loss_fn(logits, b_labels.type_as(logits))\r\n","        \r\n","    # Accumulate the validation loss.\r\n","    total_eval_loss += loss.item()\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","    label_ids = b_labels.cpu().numpy()\r\n","\r\n","    t_labels.append(label_ids)\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_labels = np.concatenate(t_labels, axis=0)\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","# Report the final accuracy for this validation run.\r\n","# print(all_dev_logits)\r\n","avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\r\n","          \r\n","print(\"Dev Accuracy: {0:.4f}\".format(avg_val_accuracy))\r\n","\r\n","xlmr_87_dev = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FSLSecUtb1uz"},"source":["**XLMR 1.2**"]},{"cell_type":"code","metadata":{"id":"fbDoL_hubkGH"},"source":["model_bert.load_state_dict(torch.load(os.path.join(path_xlmr_1_2,'roberta_check_best_dev')))\r\n","model_log_reg.load_state_dict(torch.load(os.path.join(path_xlmr_1_2, 'log_reg_check_best_dev')))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in test_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","xlmr_xlwic_test = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","        b_labels = batch[3].to(device)        \r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        loss = loss_fn(logits, b_labels.type_as(logits))\r\n","        \r\n","    # Accumulate the validation loss.\r\n","    total_eval_loss += loss.item()\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","    label_ids = b_labels.cpu().numpy()\r\n","\r\n","    t_labels.append(label_ids)\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_labels = np.concatenate(t_labels, axis=0)\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","# Report the final accuracy for this validation run.\r\n","# print(all_dev_logits)\r\n","avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\r\n","          \r\n","print(\"Dev Accuracy: {0:.4f}\".format(avg_val_accuracy))\r\n","\r\n","xlmr_xlwic_dev = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVK8t9i8cKLy"},"source":["**XLMR 1.3**"]},{"cell_type":"code","metadata":{"id":"xjvcn1YRcNBJ"},"source":["model_bert.load_state_dict(torch.load(os.path.join(path_xlmr_1_3,'roberta_check_best_dev')))\r\n","model_log_reg.load_state_dict(torch.load(os.path.join(path_xlmr_1_3, 'log_reg_check_best_dev')))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in test_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","xlmr_all_rev_test = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","        b_labels = batch[3].to(device)        \r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        loss = loss_fn(logits, b_labels.type_as(logits))\r\n","        \r\n","    # Accumulate the validation loss.\r\n","    total_eval_loss += loss.item()\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    # print(logits)\r\n","    label_ids = b_labels.cpu().numpy()\r\n","\r\n","    t_labels.append(label_ids)\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_labels = np.concatenate(t_labels, axis=0)\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","# Report the final accuracy for this validation run.\r\n","# print(all_dev_logits)\r\n","avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\r\n","          \r\n","print(\"Dev Accuracy: {0:.4f}\".format(avg_val_accuracy))\r\n","\r\n","xlmr_all_rev_dev = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzseg9_05BtQ"},"source":["Majority Vote Ensemble on Dev"]},{"cell_type":"code","metadata":{"id":"izQO9Th5ziQP"},"source":["label = []\r\n","\r\n","for i in range(len(xlmr_87_dev)):\r\n","  pos = 0\r\n","  if xlmr_87_dev[i]>0.5:\r\n","    pos = pos + 1\r\n","  if xlmr_all_rev_dev[i]>0.5:\r\n","    pos = pos + 1\r\n","  if xlmr_xlwic_dev[i]>0.5:\r\n","    pos = pos + 1\r\n","  if mbert_en_no_rev_dev[i]>0.5:\r\n","    pos = pos + 1\r\n","  if mbert_xlwic_dev[i]>0.5:\r\n","    pos = pos + 1\r\n","\r\n","  if pos<3:\r\n","    label.append('F')\r\n","  else:\r\n","    label.append('T')\r\n","  \r\n","i=0\r\n","acc=0\r\n","for i in range(0,len(label)):\r\n","\r\n","  if tags[i]==label[i]:\r\n","    acc= acc+1\r\n","acc = acc/len(label)\r\n","print(acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8H5PBcgY5HKt"},"source":["Majority Vote Ensemble on Test"]},{"cell_type":"code","metadata":{"id":"cA3j_7UW5MOB"},"source":["test_preds = []\r\n","\r\n","for i in range(len(xlmr_87_test)):\r\n","  pos = 0\r\n","  if xlmr_87_test[i]>0.5:\r\n","    pos = pos + 1\r\n","  if xlmr_all_rev_test[i]>0.5:\r\n","    pos = pos + 1\r\n","  if xlmr_xlwic_test[i]>0.5:\r\n","    pos = pos + 1\r\n","  if mbert_en_no_rev_test[i]>0.5:\r\n","    pos = pos + 1\r\n","  if mbert_xlwic_test[i]>0.5:\r\n","    pos = pos + 1\r\n","\r\n","  if pos<3:\r\n","    test_preds.append('F')\r\n","  else:\r\n","    test_preds.append('T')\r\n","\r\n","print(\"TEST PREDS ARE IN 'test_preds'\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fFEECoVUjvA"},"source":["# SAVE TEST PREDS IF U WANT TO\r\n","\r\n","p = pd.Datafram(test_preds, columns=['tag'])\r\n","p.to_csv(\"ENTER PATH\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VhlJQIJW_wld"},"source":["italicized text##**5.2** English Inference\r\n","\r\n","---\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"IGfZ-NKrAN2l"},"source":["import os\r\n","import pandas as pd\r\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnkHTCCdAV8d"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive/',)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGDgZM37AXX2"},"source":["train_df = pd.read_csv(\"/content/drive/MyDrive/datasets/sub/dev_en_rev.csv\")\r\n","train_df.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\r\n","\r\n","dev_df = pd.read_csv(\"/content/drive/MyDrive/datasets/MCL-WiC/evaluationtestset/ENGLISH/test.csv\")\r\n","dev_df.rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"}, inplace=True)\r\n","tags = train_df['tag'].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-ZnqVRwBUv6"},"source":["import torch\r\n","\r\n","# If there's a GPU available...\r\n","if torch.cuda.is_available():    \r\n","\r\n","    # Tell PyTorch to use the GPU.    \r\n","    device = torch.device(\"cuda\")\r\n","\r\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n","\r\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n","\r\n","# If not...\r\n","else:\r\n","    print('No GPU available, using the CPU instead.')\r\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyxzZsJvBWYx"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atdGy03xBYQJ"},"source":["# Adding a signal without spaces\r\n","train_sentences_1 =  []\r\n","train_sentences_2 = []\r\n","\r\n","dev_sentences_1 =  []\r\n","dev_sentences_2 = []\r\n","\r\n","list_train_sentences_1 = list(train_df['sent1'])\r\n","list_train_sentences_2 = list(train_df['sent2'])\r\n","\r\n","list_dev_sentences_1 =  list(dev_df['sent1'])\r\n","list_dev_sentences_2 = list(dev_df['sent2'])\r\n","\r\n","for i in range(len(list_train_sentences_1)):\r\n","  sentence_1 = list_train_sentences_1[i]\r\n","  # print(sentence_1)\r\n","  s1 = int(train_df['start1'][i])\r\n","  e1 = int(train_df['end1'][i])\r\n","  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\r\n","  train_sentences_1.append(sentence_1)\r\n","\r\n","  sentence_2 = list_train_sentences_2[i]\r\n","  # print(sentence_2)\r\n","  s2 = int(train_df['start2'][i])\r\n","  e2 = int(train_df['end2'][i])\r\n","  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\r\n","  train_sentences_2.append(sentence_2)\r\n","  # print((train_sentences_1[i], train_sentences_2[i]))\r\n","\r\n","for i in range(len(list_dev_sentences_1)):\r\n","  d_sentence_1 = list_dev_sentences_1[i]\r\n","  # print(dev_data_df['start1'][i])\r\n","  s1 = int(dev_df['start1'][i])\r\n","  e1 = int(dev_df['end1'][i])\r\n","  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\r\n","  dev_sentences_1.append(d_sentence_1)\r\n","\r\n","  d_sentence_2 = list_dev_sentences_2[i]\r\n","  s2 = int(dev_df['start2'][i])\r\n","  e2 = int(dev_df['end2'][i])\r\n","  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\r\n","  dev_sentences_2.append(d_sentence_2)\r\n","\r\n","\r\n","print((train_sentences_2[:10]))\r\n","print((train_sentences_1[:10]))\r\n","print(dev_sentences_1[:10])\r\n","print(dev_sentences_2[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ow8bzhqBBa_x"},"source":["XLMR_MIX_MCL\r\n"]},{"cell_type":"code","metadata":{"id":"F_8HA4xdBaTt"},"source":["#xlmr_mcl\r\n","from transformers import XLMRobertaTokenizerFast\r\n","\r\n","# Load the BERT tokenizer.\r\n","print('Loading roberta tokenizer...')\r\n","tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\r\n","\r\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_train['input_ids'][0]))\r\n","print(type(encoded_inputs_train['input_ids']))\r\n","\r\n","\r\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_dev['input_ids'][0]))\r\n","print(type(encoded_inputs_dev['input_ids']))\r\n","\r\n","# create wordpeice indices of the words of interest now\r\n","# be aware, that due to signal the actual token have their positions offsetted\r\n","train_pos_1 = []\r\n","train_pos_2 = []\r\n","\r\n","for j in range(len(train_sentences_1)):\r\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(train_df['start1'][j])\r\n","  s2 = int(train_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  train_pos_1.append(pos1)\r\n","  train_pos_2.append(pos2)\r\n","\r\n","dev_pos_1 = []\r\n","dev_pos_2 = []\r\n","\r\n","for j in range(len(dev_sentences_1)):\r\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(dev_df['start1'][j])\r\n","  s2 = int(dev_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  dev_pos_1.append(pos1)\r\n","  dev_pos_2.append(pos2)\r\n","\r\n","train_pos_1 = torch.LongTensor(train_pos_1)\r\n","train_pos_2 = torch.LongTensor(train_pos_2)\r\n","\r\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\r\n","\r\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\r\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\r\n","\r\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\r\n","\r\n","from torch.utils.data import TensorDataset\r\n","\r\n","# Combine the training inputs into a TensorDataset.\r\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\r\n","                              encoded_inputs_train['attention_mask'], train_pos)\r\n","\r\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\r\n","                              encoded_inputs_dev['attention_mask'], dev_pos)\r\n","\r\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","# The DataLoader needs to know our batch size for training, so we specify it \r\n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n","# size of 16 or 32.\r\n","batch_size = 8\r\n","\r\n","# Create the DataLoaders for our training and validation sets.\r\n","# We'll take training samples in random order. \r\n","train_dataloader = DataLoader(\r\n","            train_dataset,  # The training samples.\r\n","            sampler = SequentialSampler(train_dataset), # Select batches randomly\r\n","            batch_size = batch_size # Trains with this batch size.\r\n","        )\r\n","\r\n","# For validation the order doesn't matter, so we'll just read them sequentially.\r\n","dev_dataloader = DataLoader(\r\n","            dev_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\r\n","            batch_size = batch_size # Evaluate with this batch size.\r\n","        )\r\n","\r\n","from transformers import XLMRobertaModel, AdamW, BertConfig\r\n","import torch.nn as nn\r\n","\r\n","class BERTi(nn.Module):\r\n","\r\n","    def __init__(self):\r\n","        super(BERTi, self).__init__()\r\n","\r\n","        options_name = \"xlm-roberta-large\"\r\n","        hidden_states = False\r\n","        self.encoder = XLMRobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\r\n","\r\n","    def forward(self, input_ids, attention_mask):\r\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\r\n","\r\n","        return last_layer\r\n","\r\n","class Logistic_Reg(nn.Module):\r\n","\r\n","  def __init__(self):\r\n","    super(Logistic_Reg, self).__init__()\r\n","\r\n","    self.fc1 = nn.Linear(2048, 1)\r\n","\r\n","  def forward(self, x):\r\n","    x = self.fc1(x)\r\n","\r\n","    return x\r\n","\r\n","model_bert = BERTi().to(device)\r\n","model_log_reg = Logistic_Reg().to(device)\r\n","\r\n","model_bert.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/Mix/xlmr_mix_no_en_rev/roberta_check_best_dev'))\r\n","model_log_reg.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/Mix/xlmr_mix_no_en_rev/log_reg_check_best_dev'))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in train_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)    \r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","xlmr_mix_mcl_rev_dev_logits = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","xlmr_mix_mcl_rev_test_logits = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HV1ib8-gBkOG"},"source":["XLMR_MCL"]},{"cell_type":"code","metadata":{"id":"sN_gVhQuBm2c"},"source":["#xlmr_mcl\r\n","from transformers import XLMRobertaTokenizerFast\r\n","\r\n","# Load the BERT tokenizer.\r\n","print('Loading roberta tokenizer...')\r\n","tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\r\n","\r\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_train['input_ids'][0]))\r\n","print(type(encoded_inputs_train['input_ids']))\r\n","\r\n","\r\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_dev['input_ids'][0]))\r\n","print(type(encoded_inputs_dev['input_ids']))\r\n","\r\n","# create wordpeice indices of the words of interest now\r\n","# be aware, that due to signal the actual token have their positions offsetted\r\n","train_pos_1 = []\r\n","train_pos_2 = []\r\n","\r\n","for j in range(len(train_sentences_1)):\r\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(train_df['start1'][j])\r\n","  s2 = int(train_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  train_pos_1.append(pos1)\r\n","  train_pos_2.append(pos2)\r\n","\r\n","\r\n","dev_pos_1 = []\r\n","dev_pos_2 = []\r\n","\r\n","for j in range(len(dev_sentences_1)):\r\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(dev_df['start1'][j])\r\n","  s2 = int(dev_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  dev_pos_1.append(pos1)\r\n","  dev_pos_2.append(pos2)\r\n","\r\n","train_pos_1 = torch.LongTensor(train_pos_1)\r\n","train_pos_2 = torch.LongTensor(train_pos_2)\r\n","\r\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\r\n","\r\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\r\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\r\n","\r\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\r\n","\r\n","from torch.utils.data import TensorDataset\r\n","\r\n","# Combine the training inputs into a TensorDataset.\r\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\r\n","                              encoded_inputs_train['attention_mask'], train_pos)\r\n","\r\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\r\n","                              encoded_inputs_dev['attention_mask'], dev_pos)\r\n","\r\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","# The DataLoader needs to know our batch size for training, so we specify it \r\n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n","# size of 16 or 32.\r\n","batch_size = 8\r\n","\r\n","# Create the DataLoaders for our training and validation sets.\r\n","# We'll take training samples in random order. \r\n","train_dataloader = DataLoader(\r\n","            train_dataset,  # The training samples.\r\n","            sampler = SequentialSampler(train_dataset), # Select batches randomly\r\n","            batch_size = batch_size # Trains with this batch size.\r\n","        )\r\n","\r\n","# For validation the order doesn't matter, so we'll just read them sequentially.\r\n","dev_dataloader = DataLoader(\r\n","            dev_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\r\n","            batch_size = batch_size # Evaluate with this batch size.\r\n","        )\r\n","\r\n","from transformers import XLMRobertaModel, AdamW, BertConfig\r\n","import torch.nn as nn\r\n","\r\n","class BERTi(nn.Module):\r\n","\r\n","    def __init__(self):\r\n","        super(BERTi, self).__init__()\r\n","\r\n","        options_name = \"xlm-roberta-large\"\r\n","        hidden_states = False\r\n","        self.encoder = XLMRobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\r\n","\r\n","    def forward(self, input_ids, attention_mask):\r\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\r\n","\r\n","        return last_layer\r\n","\r\n","class Logistic_Reg(nn.Module):\r\n","\r\n","  def __init__(self):\r\n","    super(Logistic_Reg, self).__init__()\r\n","\r\n","    self.fc1 = nn.Linear(2048, 1)\r\n","\r\n","  def forward(self, x):\r\n","    x = self.fc1(x)\r\n","\r\n","    return x\r\n","\r\n","model_bert = BERTi().to(device)\r\n","model_log_reg = Logistic_Reg().to(device)\r\n","\r\n","\r\n","\r\n","model_bert.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/xlmr_mcl_rev/xlmr_check_best_dev'))\r\n","model_log_reg.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/xlmr_mcl_rev/log_reg_check_best_dev'))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in train_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","xlmr_mcl_rev_dev_logits = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","xlmr_mcl_rev_test_logits = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRPnfXswBqg-"},"source":["Roberta_MCL"]},{"cell_type":"code","metadata":{"id":"LUyNRuPsByFk"},"source":["#roberta_large_mcl\r\n","\r\n","from transformers import RobertaTokenizerFast\r\n","\r\n","# Load the BERT tokenizer.\r\n","print('Loading roberta tokenizer...')\r\n","tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\r\n","\r\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_train['input_ids'][0]))\r\n","print(type(encoded_inputs_train['input_ids']))\r\n","\r\n","\r\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_dev['input_ids'][0]))\r\n","print(type(encoded_inputs_dev['input_ids']))\r\n","\r\n","# create wordpeice indices of the words of interest now\r\n","# be aware, that due to signal the actual token have their positions offsetted\r\n","train_pos_1 = []\r\n","train_pos_2 = []\r\n","\r\n","for j in range(len(train_sentences_1)):\r\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(train_df['start1'][j])\r\n","  s2 = int(train_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  train_pos_1.append(pos1)\r\n","  train_pos_2.append(pos2)\r\n","\r\n","\r\n","dev_pos_1 = []\r\n","dev_pos_2 = []\r\n","\r\n","for j in range(len(dev_sentences_1)):\r\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(dev_df['start1'][j])\r\n","  s2 = int(dev_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  dev_pos_1.append(pos1)\r\n","  dev_pos_2.append(pos2)\r\n","\r\n","train_pos_1 = torch.LongTensor(train_pos_1)\r\n","train_pos_2 = torch.LongTensor(train_pos_2)\r\n","\r\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\r\n","\r\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\r\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\r\n","\r\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\r\n","\r\n","from torch.utils.data import TensorDataset\r\n","\r\n","# Combine the training inputs into a TensorDataset.\r\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'],\r\n","                              encoded_inputs_train['attention_mask'], train_pos)\r\n","\r\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'],\r\n","                              encoded_inputs_dev['attention_mask'], dev_pos)\r\n","\r\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","# The DataLoader needs to know our batch size for training, so we specify it \r\n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n","# size of 16 or 32.\r\n","batch_size = 8\r\n","\r\n","# Create the DataLoaders for our training and validation sets.\r\n","# We'll take training samples in random order. \r\n","train_dataloader = DataLoader(\r\n","            train_dataset,  # The training samples.\r\n","            sampler = SequentialSampler(train_dataset), # Select batches randomly\r\n","            batch_size = batch_size # Trains with this batch size.\r\n","        )\r\n","\r\n","# For validation the order doesn't matter, so we'll just read them sequentially.\r\n","dev_dataloader = DataLoader(\r\n","            dev_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\r\n","            batch_size = batch_size # Evaluate with this batch size.\r\n","        )\r\n","\r\n","from transformers import RobertaModel, AdamW, BertConfig\r\n","import torch.nn as nn\r\n","\r\n","class BERTi(nn.Module):\r\n","\r\n","    def __init__(self):\r\n","        super(BERTi, self).__init__()\r\n","\r\n","        options_name = \"roberta-large\"\r\n","        hidden_states = False\r\n","        self.encoder = RobertaModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\r\n","\r\n","    def forward(self, input_ids, attention_mask):\r\n","        last_layer, _ = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\r\n","\r\n","        return last_layer\r\n","\r\n","class Logistic_Reg(nn.Module):\r\n","\r\n","  def __init__(self):\r\n","    super(Logistic_Reg, self).__init__()\r\n","\r\n","    self.fc1 = nn.Linear(2048, 1)\r\n","\r\n","  def forward(self, x):\r\n","    x = self.fc1(x)\r\n","\r\n","    return x\r\n","\r\n","model_bert = BERTi().to(device)\r\n","model_log_reg = Logistic_Reg().to(device)\r\n","\r\n","\r\n","model_bert.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/roberta_signal_mcl_rev/roberta_check_best_dev'))\r\n","model_log_reg.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/roberta_signal_mcl_rev/log_reg_check_best_dev'))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_preds = []\r\n","\r\n","for batch in train_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","roberta_signal_mcl_rev_dev_logits = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","roberta_signal_mcl_rev_test_logits = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ytl2x4-TBtpf"},"source":["Roberta_Augmented"]},{"cell_type":"code","metadata":{"id":"JH051N60B26G"},"source":["#roberta_large_aug\r\n","\r\n","model_bert = BERTi().to(device)\r\n","model_log_reg = Logistic_Reg().to(device)\r\n","\r\n","\r\n","model_bert.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/roberta_signal_aug_rev/roberta_check_best_dev'))\r\n","model_log_reg.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/roberta_signal_aug_rev/log_reg_check_best_dev'))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","# Tracking variables \r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_preds = []\r\n","\r\n","for batch in train_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","roberta_signal_aug_rev_dev_logits = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","\r\n","    with torch.no_grad():\r\n","        b_input_ids = batch[0].to(device)\r\n","        b_attention_mask = batch[1].to(device)\r\n","        b_poses = batch[2].to(device)\r\n","\r\n","        last_layer = model_bert(b_input_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","roberta_signal_aug_rev_test_logits = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uo7ag_G2__YY"},"source":["BERT_Large"]},{"cell_type":"code","metadata":{"id":"fR4oKEsqB9cw"},"source":["#bert large\r\n","from transformers import BertTokenizerFast\r\n","\r\n","# Load the BERT tokenizer.\r\n","print('Loading BERT tokenizer...')\r\n","tokenizer = BertTokenizerFast.from_pretrained('bert-large-cased')\r\n","\r\n","encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_train['input_ids'][0]))\r\n","print(type(encoded_inputs_train['input_ids']))\r\n","\r\n","\r\n","encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\r\n","print(len(encoded_inputs_dev['input_ids'][0]))\r\n","print(type(encoded_inputs_dev['input_ids']))\r\n","\r\n","# create wordpeice indices of the words of interest now\r\n","# be aware, that due to signal the actual token have their positions offsetted\r\n","train_pos_1 = []\r\n","train_pos_2 = []\r\n","\r\n","for j in range(len(train_sentences_1)):\r\n","  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(train_df['start1'][j])\r\n","  s2 = int(train_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  train_pos_1.append(pos1)\r\n","  train_pos_2.append(pos2)\r\n","\r\n","dev_pos_1 = []\r\n","dev_pos_2 = []\r\n","\r\n","for j in range(len(dev_sentences_1)):\r\n","  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\r\n","  second_sent = False\r\n","  s1 = int(dev_df['start1'][j])\r\n","  s2 = int(dev_df['start2'][j])\r\n","\r\n","  for i, offset in enumerate(offsets):\r\n","    if i == 0:\r\n","      continue #cls\r\n","    if offset[1] == 0:\r\n","      second_sent = True\r\n","\r\n","    if offset[0] == s1+1 and second_sent == False:\r\n","      pos1 = i\r\n","    elif offset[0] == s2+1 and second_sent == True:\r\n","      pos2 = i\r\n","      break\r\n","  dev_pos_1.append(pos1)\r\n","  dev_pos_2.append(pos2)\r\n","\r\n","train_pos_1 = torch.LongTensor(train_pos_1)\r\n","train_pos_2 = torch.LongTensor(train_pos_2)\r\n","\r\n","train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\r\n","\r\n","dev_pos_1 = torch.LongTensor(dev_pos_1)\r\n","dev_pos_2 = torch.LongTensor(dev_pos_2)\r\n","\r\n","dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\r\n","\r\n","from torch.utils.data import TensorDataset\r\n","\r\n","# Combine the training inputs into a TensorDataset.\r\n","train_dataset = TensorDataset(encoded_inputs_train['input_ids'], encoded_inputs_train['token_type_ids'],\r\n","                              encoded_inputs_train['attention_mask'], train_pos)\r\n","\r\n","dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'], encoded_inputs_dev['token_type_ids'],\r\n","                              encoded_inputs_dev['attention_mask'], dev_pos)\r\n","\r\n","\r\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n","\r\n","# The DataLoader needs to know our batch size for training, so we specify it \r\n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n","# size of 16 or 32.\r\n","batch_size = 8\r\n","\r\n","# Create the DataLoaders for our training and validation sets.\r\n","# We'll take training samples in random order. \r\n","train_dataloader = DataLoader(\r\n","            train_dataset,  # The training samples.\r\n","            sampler = SequentialSampler(train_dataset), # Select batches randomly\r\n","            batch_size = batch_size # Trains with this batch size.\r\n","        )\r\n","\r\n","# For validation the order doesn't matter, so we'll just read them sequentially.\r\n","dev_dataloader = DataLoader(\r\n","            dev_dataset, # The validation samples.\r\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\r\n","            batch_size = batch_size # Evaluate with this batch size.\r\n","        )\r\n","\r\n","from transformers import BertModel, AdamW, BertConfig\r\n","import torch.nn as nn\r\n","\r\n","class BERTi(nn.Module):\r\n","\r\n","    def __init__(self):\r\n","        super(BERTi, self).__init__()\r\n","\r\n","        options_name = \"bert-large-cased\"\r\n","        hidden_states = False\r\n","        self.encoder = BertModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\r\n","\r\n","    def forward(self, input_ids, token_type_ids, attention_mask):\r\n","        last_layer, _ = self.encoder(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\r\n","\r\n","        return last_layer\r\n","\r\n","class Logistic_Reg(nn.Module):\r\n","\r\n","  def __init__(self):\r\n","    super(Logistic_Reg, self).__init__()\r\n","\r\n","    self.fc1 = nn.Linear(2048, 1)\r\n","\r\n","  def forward(self, x):\r\n","    x = self.fc1(x)\r\n","\r\n","    return x\r\n","\r\n","model_bert = BERTi().to(device)\r\n","model_log_reg = Logistic_Reg().to(device)\r\n","\r\n","model_bert.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/bert_large_mcl_rev_dev/model_bert_dev_best'))\r\n","model_log_reg.load_state_dict(torch.load('/content/drive/MyDrive/datasets/sub/models/English/bert_large_mcl_rev_dev/model_log_reg_dev_best'))\r\n","\r\n","model_bert.eval()\r\n","model_log_reg.eval()\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_labels = []\r\n","t_preds = []\r\n","\r\n","for batch in train_dataloader:\r\n","    b_input_ids = batch[0].to(device)\r\n","    b_token_type_ids = batch[1].to(device)\r\n","    b_attention_mask = batch[2].to(device)\r\n","    b_poses = batch[3].to(device)\r\n","    \r\n","    # Tell pytorch not to bother with constructing the compute graph during\r\n","    # the forward pass, since this is only needed for backprop (training).\r\n","    with torch.no_grad():        \r\n","\r\n","        last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","        \r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    \r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","bert_large_mcl_rev_dev_logits = 1 / (1 + np.exp(-all_dev_logits))\r\n","\r\n","\r\n","total_eval_accuracy = 0\r\n","total_eval_loss = 0\r\n","nb_eval_steps = 0\r\n","\r\n","t_preds = []\r\n","for batch in dev_dataloader:\r\n","    b_input_ids = batch[0].to(device)\r\n","    b_token_type_ids = batch[1].to(device)\r\n","    b_attention_mask = batch[2].to(device)\r\n","    b_poses = batch[3].to(device)\r\n","    \r\n","    # Tell pytorch not to bother with constructing the compute graph during\r\n","    # the forward pass, since this is only needed for backprop (training).\r\n","    with torch.no_grad():        \r\n","\r\n","        last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\r\n","        b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\r\n","        gathered_activations = torch.gather(last_layer, 1, b_poses)\r\n","        logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\r\n","\r\n","    # Move logits and labels to CPU\r\n","    logits = logits.cpu().detach().numpy()\r\n","    t_preds.append(logits)\r\n","    \r\n","\r\n","all_dev_logits = np.concatenate(t_preds, axis=0)\r\n","\r\n","bert_large_mcl_rev_test_logits = 1 / (1 + np.exp(-all_dev_logits))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JXb7IpwOCCB9"},"source":["Majority Ensemble"]},{"cell_type":"code","metadata":{"id":"1RTm-26aCFQS"},"source":["#dev\r\n","label = []\r\n","for i in range(0,len(xlmr_mcl_rev_dev_logits)):\r\n","  pos=0\r\n","  if xlmr_mcl_rev_dev_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if bert_large_mcl_rev_dev_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if roberta_signal_mcl_rev_dev_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if roberta_signal_aug_rev_dev_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if xlmr_mix_mcl_rev_dev_logits[i]>0.5:\r\n","    pos = pos+1\r\n","\r\n","  if pos<3:\r\n","    label.append('F')\r\n","  else:\r\n","    label.append('T')\r\n","       \r\n","acc=0\r\n","for i in range(0,len(label)):\r\n","  if tags[i]==label[i]:\r\n","    acc= acc+1\r\n","acc = acc/len(label)\r\n","print(acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNAAt_cPCLOC"},"source":["#test\r\n","label = []\r\n","for i in range(0,len(xlmr_mcl_rev_test_logits)):\r\n","  pos = 0\r\n","  if xlmr_mcl_rev_test_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if bert_large_mcl_rev_test_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if roberta_signal_mcl_rev_test_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if roberta_signal_aug_rev_test_logits[i]>0.5:\r\n","    pos = pos+1\r\n","  if xlmr_mix_mcl_rev_test_logits[i]>0.5:\r\n","    pos = pos+1\r\n","\r\n","  if pos<3:\r\n","    label.append('F')\r\n","  else:\r\n","    label.append('T')\r\n","\r\n","dev_df['tag'] = label\r\n","dev_df.head()\r\n","dev_df.to_csv(\"/content/drive/My Drive/datasets/sub/predictions/preds_english.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rob5s9iZCSnI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yxUa1v5ZDz8m"},"source":["## 6. Evaluating given labels "]},{"cell_type":"code","metadata":{"id":"4r--p9yBU8a3"},"source":["# ENTER CSV PATHS OF THE FOLLOWING\r\n","# OUR PREDS CAN BE DOWLOADED FROM THE LINK GIVEN IN README"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jOVUi2duhSE"},"source":["pred_english_path = \"\"\r\n","pred_non_english_path = \"\" \r\n","label_english_path = \"\"\r\n","label_arabic_path = \"\"\r\n","label_chinese_path = \"\"\r\n","label_russian_path = \"\"\r\n","label_french_path = \"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kugiO0q1vBTY"},"source":["pred_english_df = pd.read_csv(pred_english_path)\r\n","pred_non_english_df = pd.read_csv(pred_non_english_path)\r\n","\r\n","label_english_df = pd.read_csv(label_english_path)\r\n","label_arabic_df = pd.read_csv(label_arabic_path)\r\n","label_chinese_df = pd.read_csv(label_chinese_path)\r\n","label_russian_df = pd.read_csv(label_russian_path)\r\n","label_french_df = pd.read_csv(label_french_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLYMQU97vsVk"},"source":["tags_pred_english = pred_english_df['tag'].replace({'F' : 0, 'T' : 1}).to_numpy()\r\n","tags_pred_non_english = pred_non_english_df['tag'].replace({'F' : 0, 'T' : 1}).to_numpy()\r\n","\r\n","tags_label_english = label_english_df['tag'].replace({'F' : 0, 'T' : 1}).to_numpy()\r\n","tags_label_arabic = label_arabic_df['tag'].replace({'F' : 0, 'T' : 1}).to_numpy()\r\n","tags_label_chinese = label_chinese_df['tag'].replace({'F' : 0, 'T' : 1}).to_numpy()\r\n","tags_label_russian = label_russian_df['tag'].replace({'F' : 0, 'T' : 1}).to_numpy()\r\n","tags_label_french = label_french_df['tag'].replace({'F' : 0, 'T' : 1}).to_numpy()\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vBCQE7OKvnOX"},"source":["# english accuracy\r\n","\r\n","print(\"Accuracy on English = \"+str((tags_label_english == tags_pred_english).sum()/len(tags_label_english)))\r\n","\r\n","print(\"Accuracy on Arabic = \"+str((tags_label_arabic == tags_pred_non_english[400:600]).sum()/len(tags_label_arabic)))\r\n","\r\n","print(\"Accuracy on Chinese = \"+str((tags_label_chinese == tags_pred_non_english[200:400]).sum()/len(tags_label_chinese)))\r\n","\r\n","print(\"Accuracy on French = \"+str((tags_label_french == tags_pred_non_english[600:800]).sum()/len(tags_label_french)))\r\n","\r\n","print(\"Accuracy on Russian = \"+str((tags_label_russian == tags_pred_non_english[:200]).sum()/len(tags_label_russian)))\r\n"],"execution_count":null,"outputs":[]}]}