{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELECTRA_finetuning_en_en_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cbd3ad0565a14343b858ff6348811864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9913d7a313a241448c88790afe5657b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1e54bbf5f74b4504b3eccdd40f8c3984",
              "IPY_MODEL_dbed5f3301734dd6ab50af2873e16f9a"
            ]
          }
        },
        "9913d7a313a241448c88790afe5657b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e54bbf5f74b4504b3eccdd40f8c3984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1dbd02ba10e24e0682d3bda11f2bddbc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_897ab20aced64ecda0784fbe3260b1a5"
          }
        },
        "dbed5f3301734dd6ab50af2873e16f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79b74de8a78a4f44802e3d12a99279fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:06&lt;00:00, 33.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ee36abd25534ef7944d9dcdf5a886c4"
          }
        },
        "1dbd02ba10e24e0682d3bda11f2bddbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "897ab20aced64ecda0784fbe3260b1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79b74de8a78a4f44802e3d12a99279fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ee36abd25534ef7944d9dcdf5a886c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c19e119dd25b43b9a4343a12857cf24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e79ef91420e402da1b29e5e56a0d0b7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ff598df472704d5cbd4f6d772d6f5569",
              "IPY_MODEL_edc94b8be7f5423688077097bf038b2e"
            ]
          }
        },
        "4e79ef91420e402da1b29e5e56a0d0b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff598df472704d5cbd4f6d772d6f5569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_74bfc31ddc3746ac8b734fc1ddd2e221",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5240b34365444a18ea2bf4a1acadc5e"
          }
        },
        "edc94b8be7f5423688077097bf038b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0679678e75a04d028e220978b66c0797",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 893kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f64abf9e2bc412b8f69dc2908f15032"
          }
        },
        "74bfc31ddc3746ac8b734fc1ddd2e221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5240b34365444a18ea2bf4a1acadc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0679678e75a04d028e220978b66c0797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f64abf9e2bc412b8f69dc2908f15032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d8b29ccaacc4b18a88581e426af29ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_38ca88a520cc43f6b313f697cd329329",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e3631cb104e74bc1b8d373ccd9455491",
              "IPY_MODEL_8c6857a297824765adb07aa9b6fd8108"
            ]
          }
        },
        "38ca88a520cc43f6b313f697cd329329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3631cb104e74bc1b8d373ccd9455491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f06b975f83db40ba8b02fa71592e74f9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 469,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 469,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9af27b16d0084d868dcd1c0da00f64e2"
          }
        },
        "8c6857a297824765adb07aa9b6fd8108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_96cbeca929524ba2ba07dadad4040cb2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 469/469 [00:01&lt;00:00, 314B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7b50ea253654ac2a0555150b73c4038"
          }
        },
        "f06b975f83db40ba8b02fa71592e74f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9af27b16d0084d868dcd1c0da00f64e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96cbeca929524ba2ba07dadad4040cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7b50ea253654ac2a0555150b73c4038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "046212b13cd248b1b500b4df1a735e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_269f332e2bb04f849b09160a24e3f316",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7b9818d75c4b42dd881f52020cac0ff8",
              "IPY_MODEL_d83c4c09a1df45e2a066398e056f7765"
            ]
          }
        },
        "269f332e2bb04f849b09160a24e3f316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b9818d75c4b42dd881f52020cac0ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0d4ae9002e544679fa74db1402400ed",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1344867008,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1344867008,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_883739284f8f474e9d14943b1697af68"
          }
        },
        "d83c4c09a1df45e2a066398e056f7765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e708e87c9024d98832bf22b29246737",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:27&lt;00:00, 48.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae61160e119841b6a9674a8efa0589bb"
          }
        },
        "a0d4ae9002e544679fa74db1402400ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "883739284f8f474e9d14943b1697af68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e708e87c9024d98832bf22b29246737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae61160e119841b6a9674a8efa0589bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xya_BS1ic9q"
      },
      "source": [
        "import os\n",
        "# import pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0Qy2pr4CnkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a58e76c-32b8-4737-cc63-b9a50516cfcf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzEeI0fUNJcG"
      },
      "source": [
        "mcl_path  = \"/content/drive/My Drive/datasets/MCL-WiC\"\n",
        "patht = \"/content/drive/My Drive/datasets/WiC_dataset\"\n",
        "train_path = \"/content/drive/My Drive/datasets/MCL-WiC/training_new_format.csv\"\n",
        "dev_path = \"/content/drive/My Drive/datasets/MCL-WiC/new_dev_wic_format.csv\"\n",
        "aug_path = \"/content/drive/MyDrive/datasets/AuSemCor/ausemcor_supersense_pruned.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvQ33K0jOjWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8224ab3-d73b-4ed1-fcf0-f4b49254c57f"
      },
      "source": [
        "! ls \"/content/drive/My Drive/datasets/Split_WiC_dataset\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine_Sim  dev  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZslOtNoSby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9ba0bb-7a24-46a3-c15e-a6310db72de4"
      },
      "source": [
        "# Load the wic data properly everything aaaaaaa\n",
        "\n",
        "train_wic_df = pd.read_csv(\"/content/drive/MyDrive/datasets/WiC_dataset/wic_train_new_format.csv\")\n",
        "print(train_wic_df)\n",
        "train_mcl_df = pd.read_csv(\"/content/drive/MyDrive/datasets/final/train_rev_mcl.csv\")\n",
        "# train_mcl_df_temp.columns = ['id', 'lemma', 'pos', 'sent1', 'sent2', 'start1', 'end1', 'start2', 'end2', 'tag', 'position1', 'position2']\n",
        "# train_mcl_df = train_mcl_df_temp[['lemma', 'pos', 'position1', 'start1', 'end1', 'position2', 'start2', 'end2', 'sent1', 'sent2', 'tag']]\n",
        "print(train_mcl_df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         lemma pos  ...                                              sent2  tag\n",
            "0        carry   V  ...                    Sound carries well over water .    F\n",
            "1           go   V  ...   Do you think the sofa will go through the door ?    F\n",
            "2        break   V  ...  The wholesaler broke the container loads into ...    F\n",
            "3          cup   N  ...            Bees filled the waxen cups with honey .    T\n",
            "4      academy   N  ...                               The French Academy .    F\n",
            "...        ...  ..  ...                                                ...  ...\n",
            "5423     krona   N  ...    Kronas kurss â€” the exchange rate of the krona .    T\n",
            "5424  conflict   N  ...  The conflict between the government and the re...    T\n",
            "5425    answer   V  ...                                Answer a question .    T\n",
            "5426      play   V  ...                                   Play the races .    T\n",
            "5427  invasion   N  ...                     An invasion of mobile phones .    T\n",
            "\n",
            "[5428 rows x 11 columns]\n",
            "            lemma   pos  ...  tag                    id\n",
            "0            play  NOUN  ...    F      training.en-en.0\n",
            "1            play  NOUN  ...    F      training.en-en.1\n",
            "2          esteem  NOUN  ...    T      training.en-en.2\n",
            "3          esteem  NOUN  ...    T      training.en-en.3\n",
            "4          holder  NOUN  ...    T      training.en-en.4\n",
            "...           ...   ...  ...  ...                   ...\n",
            "15995  positivity  NOUN  ...    T  training.en-en.15995\n",
            "15996     backing  NOUN  ...    F  training.en-en.15996\n",
            "15997     backing  NOUN  ...    T  training.en-en.15997\n",
            "15998        gird  VERB  ...    F  training.en-en.15998\n",
            "15999        gird  VERB  ...    T  training.en-en.15999\n",
            "\n",
            "[16000 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prozdjyh8RIp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "5d00b5ef-ed0f-439e-fd74-87d32cc9d251"
      },
      "source": [
        "train_ausem_df = pd.read_csv(\"/content/drive/MyDrive/datasets/AuSemCor/ausemcor_supersense_pruned.csv\").rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\n",
        "# train_ausem_df.columns = ['lemma', 'pos', 'sent1', 'sent2', 'start1', 'end1', 'start2', 'end2', 'tag', 'position1', 'position2' ]\n",
        "print(train_ausem_df)\n",
        "train_ausem_df.head()\n",
        "# train_ausem_df.to_csv(\"/content/drive/MyDrive/datasets/AuSemCor/10jan_all_lemmas_62k\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        id      lemma   pos  ... tag index1  index2\n",
            "0      training.en-en.8000         be  VERB  ...   T      3       2\n",
            "1      training.en-en.8001         be  VERB  ...   F      3       1\n",
            "2      training.en-en.8002     review  NOUN  ...   T     30      13\n",
            "3      training.en-en.8003     review  NOUN  ...   F     30       9\n",
            "4      training.en-en.8004    benefit  NOUN  ...   T     20      26\n",
            "...                    ...        ...   ...  ...  ..    ...     ...\n",
            "5477  training.en-en.13477    appease  VERB  ...   F     14      18\n",
            "5478  training.en-en.13478    uncover  VERB  ...   T     41       8\n",
            "5479  training.en-en.13479    uncover  VERB  ...   F     41      15\n",
            "5480  training.en-en.13480  subscribe  VERB  ...   T      2       8\n",
            "5481  training.en-en.13481  subscribe  VERB  ...   F      2      19\n",
            "\n",
            "[5482 rows x 12 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "      <th>sent1</th>\n",
              "      <th>sent2</th>\n",
              "      <th>start1</th>\n",
              "      <th>end1</th>\n",
              "      <th>start2</th>\n",
              "      <th>end2</th>\n",
              "      <th>tag</th>\n",
              "      <th>index1</th>\n",
              "      <th>index2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>training.en-en.8000</td>\n",
              "      <td>be</td>\n",
              "      <td>VERB</td>\n",
              "      <td>Position may not be everything, but in the cas...</td>\n",
              "      <td>The movie was The Great Train Robbery and its ...</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "      <td>T</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>training.en-en.8001</td>\n",
              "      <td>be</td>\n",
              "      <td>VERB</td>\n",
              "      <td>Position may not be everything, but in the cas...</td>\n",
              "      <td>Seigner is the dean of the company, the oldest...</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>F</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>training.en-en.8002</td>\n",
              "      <td>review</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>The action of the Commission in allowing or de...</td>\n",
              "      <td>Such certification shall be final and conclusi...</td>\n",
              "      <td>161</td>\n",
              "      <td>167</td>\n",
              "      <td>77</td>\n",
              "      <td>83</td>\n",
              "      <td>T</td>\n",
              "      <td>30</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>training.en-en.8003</td>\n",
              "      <td>review</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>The action of the Commission in allowing or de...</td>\n",
              "      <td>He opens his discourse, however, with a review...</td>\n",
              "      <td>161</td>\n",
              "      <td>167</td>\n",
              "      <td>42</td>\n",
              "      <td>48</td>\n",
              "      <td>F</td>\n",
              "      <td>30</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>training.en-en.8004</td>\n",
              "      <td>benefit</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Thus, besides the training provided to youth i...</td>\n",
              "      <td>The concentration of effective power in Rabat ...</td>\n",
              "      <td>117</td>\n",
              "      <td>125</td>\n",
              "      <td>156</td>\n",
              "      <td>164</td>\n",
              "      <td>T</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id    lemma   pos  ... tag index1  index2\n",
              "0  training.en-en.8000       be  VERB  ...   T      3       2\n",
              "1  training.en-en.8001       be  VERB  ...   F      3       1\n",
              "2  training.en-en.8002   review  NOUN  ...   T     30      13\n",
              "3  training.en-en.8003   review  NOUN  ...   F     30       9\n",
              "4  training.en-en.8004  benefit  NOUN  ...   T     20      26\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twi5seGd-3FU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "110f4255-9c7c-4551-ec5f-a6e758cead4c"
      },
      "source": [
        "# combine the two datasets\n",
        "train_df = pd.concat([train_mcl_df, train_wic_df, train_ausem_df], ignore_index=True)\n",
        "train_df = train_df[~train_df.lemma.str.contains(\"_\")].reset_index()\n",
        "print(train_df)\n",
        "train_df.head()\n",
        "#IDS ARE (NOT (NOT OK)) HUIHUI"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       index      lemma   pos  ...                    id  index1  index2\n",
            "0          0       play  NOUN  ...      training.en-en.0     NaN     NaN\n",
            "1          1       play  NOUN  ...      training.en-en.1     NaN     NaN\n",
            "2          2     esteem  NOUN  ...      training.en-en.2     NaN     NaN\n",
            "3          3     esteem  NOUN  ...      training.en-en.3     NaN     NaN\n",
            "4          4     holder  NOUN  ...      training.en-en.4     NaN     NaN\n",
            "...      ...        ...   ...  ...                   ...     ...     ...\n",
            "27131  27543    appease  VERB  ...  training.en-en.13477    14.0    18.0\n",
            "27132  27544    uncover  VERB  ...  training.en-en.13478    41.0     8.0\n",
            "27133  27545    uncover  VERB  ...  training.en-en.13479    41.0    15.0\n",
            "27134  27546  subscribe  VERB  ...  training.en-en.13480     2.0     8.0\n",
            "27135  27547  subscribe  VERB  ...  training.en-en.13481     2.0    19.0\n",
            "\n",
            "[27136 rows x 15 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "      <th>position1</th>\n",
              "      <th>start1</th>\n",
              "      <th>end1</th>\n",
              "      <th>position2</th>\n",
              "      <th>start2</th>\n",
              "      <th>end2</th>\n",
              "      <th>sent1</th>\n",
              "      <th>sent2</th>\n",
              "      <th>tag</th>\n",
              "      <th>id</th>\n",
              "      <th>index1</th>\n",
              "      <th>index2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>play</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>11.0</td>\n",
              "      <td>69</td>\n",
              "      <td>73</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10</td>\n",
              "      <td>14</td>\n",
              "      <td>In that context of coordination and integratio...</td>\n",
              "      <td>A musical play on the same subject was also st...</td>\n",
              "      <td>F</td>\n",
              "      <td>training.en-en.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>play</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>11.0</td>\n",
              "      <td>69</td>\n",
              "      <td>73</td>\n",
              "      <td>22.0</td>\n",
              "      <td>112</td>\n",
              "      <td>116</td>\n",
              "      <td>In that context of coordination and integratio...</td>\n",
              "      <td>In schools, when water is needed, it is girls ...</td>\n",
              "      <td>F</td>\n",
              "      <td>training.en-en.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>esteem</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>33</td>\n",
              "      <td>39</td>\n",
              "      <td>19.0</td>\n",
              "      <td>106</td>\n",
              "      <td>112</td>\n",
              "      <td>We would also like to convey our esteem and co...</td>\n",
              "      <td>Father Lini said that, because of that, the Un...</td>\n",
              "      <td>T</td>\n",
              "      <td>training.en-en.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>esteem</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>33</td>\n",
              "      <td>39</td>\n",
              "      <td>4.0</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>We would also like to convey our esteem and co...</td>\n",
              "      <td>This attests to the esteem and trust enjoyed b...</td>\n",
              "      <td>T</td>\n",
              "      <td>training.en-en.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>holder</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>12.0</td>\n",
              "      <td>74</td>\n",
              "      <td>81</td>\n",
              "      <td>6.0</td>\n",
              "      <td>27</td>\n",
              "      <td>33</td>\n",
              "      <td>This growth is the direct result of the increa...</td>\n",
              "      <td>A person may be either the holder of an option...</td>\n",
              "      <td>T</td>\n",
              "      <td>training.en-en.4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index   lemma   pos  position1  ...  tag                id  index1  index2\n",
              "0      0    play  NOUN       11.0  ...    F  training.en-en.0     NaN     NaN\n",
              "1      1    play  NOUN       11.0  ...    F  training.en-en.1     NaN     NaN\n",
              "2      2  esteem  NOUN        7.0  ...    T  training.en-en.2     NaN     NaN\n",
              "3      3  esteem  NOUN        7.0  ...    T  training.en-en.3     NaN     NaN\n",
              "4      4  holder  NOUN       12.0  ...    T  training.en-en.4     NaN     NaN\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZpa7eW2hDsY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "039f56b6-d29c-438c-ce34-cb591138a6d5"
      },
      "source": [
        "dev_df = pd.read_csv(\"/content/drive/MyDrive/datasets/MCL-WiC/dev_data.csv\").rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\n",
        "\n",
        "print(dev_df)\n",
        "dev_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                id         lemma   pos  ... start2 end2  tag\n",
            "0      dev.en-en.0      superior  NOUN  ...     41   50    F\n",
            "1      dev.en-en.1      superior  NOUN  ...     44   53    T\n",
            "2      dev.en-en.2  acquaintance  NOUN  ...     41   54    F\n",
            "3      dev.en-en.3  acquaintance  NOUN  ...     74   86    F\n",
            "4      dev.en-en.4       baggage  NOUN  ...      6   13    T\n",
            "..             ...           ...   ...  ...    ...  ...  ...\n",
            "995  dev.en-en.995         crash  NOUN  ...     75   80    T\n",
            "996  dev.en-en.996     calculate  VERB  ...     12   22    F\n",
            "997  dev.en-en.997     calculate  VERB  ...     11   21    F\n",
            "998  dev.en-en.998         click  VERB  ...     92   98    T\n",
            "999  dev.en-en.999         click  VERB  ...     59   66    F\n",
            "\n",
            "[1000 rows x 10 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "      <th>sent1</th>\n",
              "      <th>sent2</th>\n",
              "      <th>start1</th>\n",
              "      <th>end1</th>\n",
              "      <th>start2</th>\n",
              "      <th>end2</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dev.en-en.0</td>\n",
              "      <td>superior</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>No clause in a contract shall be interpreted a...</td>\n",
              "      <td>While fully aware that bishops and major super...</td>\n",
              "      <td>78</td>\n",
              "      <td>87</td>\n",
              "      <td>41</td>\n",
              "      <td>50</td>\n",
              "      <td>F</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dev.en-en.1</td>\n",
              "      <td>superior</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>No clause in a contract shall be interpreted a...</td>\n",
              "      <td>In Senegal too, the customs officer and his su...</td>\n",
              "      <td>78</td>\n",
              "      <td>87</td>\n",
              "      <td>44</td>\n",
              "      <td>53</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dev.en-en.2</td>\n",
              "      <td>acquaintance</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Such acquaintance is a right and not an obliga...</td>\n",
              "      <td>The complaints tend to be lodged against acqua...</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>41</td>\n",
              "      <td>54</td>\n",
              "      <td>F</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dev.en-en.3</td>\n",
              "      <td>acquaintance</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Such acquaintance is a right and not an obliga...</td>\n",
              "      <td>Sexual violence by non-partners refers to viol...</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>74</td>\n",
              "      <td>86</td>\n",
              "      <td>F</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dev.en-en.4</td>\n",
              "      <td>baggage</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>Where any baggage of any passenger contains fi...</td>\n",
              "      <td>In my baggage I had a Hungarian grammar book a...</td>\n",
              "      <td>10</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id         lemma   pos  ... start2 end2  tag\n",
              "0  dev.en-en.0      superior  NOUN  ...     41   50    F\n",
              "1  dev.en-en.1      superior  NOUN  ...     44   53    T\n",
              "2  dev.en-en.2  acquaintance  NOUN  ...     41   54    F\n",
              "3  dev.en-en.3  acquaintance  NOUN  ...     74   86    F\n",
              "4  dev.en-en.4       baggage  NOUN  ...      6   13    T\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaFlib8sPmIT",
        "outputId": "7f3bf4bf-1dc0-4dd7-c2cf-cbcb3ff13ab6"
      },
      "source": [
        "dev_rev_df = dev_df.copy().rename(columns = {\"sent1\":\"sent2\", \"sent2\":\"sent1\", 'start1':'start2', 'start2':'start1', 'end1':'end2', 'end2':'end1'})\r\n",
        "dev_df = pd.concat([dev_df, dev_rev_df], ignore_index=True)\r\n",
        "print(dev_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 id         lemma   pos  ... start2 end2  tag\n",
            "0       dev.en-en.0      superior  NOUN  ...     41   50    F\n",
            "1       dev.en-en.1      superior  NOUN  ...     44   53    T\n",
            "2       dev.en-en.2  acquaintance  NOUN  ...     41   54    F\n",
            "3       dev.en-en.3  acquaintance  NOUN  ...     74   86    F\n",
            "4       dev.en-en.4       baggage  NOUN  ...      6   13    T\n",
            "...             ...           ...   ...  ...    ...  ...  ...\n",
            "1995  dev.en-en.995         crash  NOUN  ...    128  133    T\n",
            "1996  dev.en-en.996     calculate  VERB  ...     99  109    F\n",
            "1997  dev.en-en.997     calculate  VERB  ...     99  109    F\n",
            "1998  dev.en-en.998         click  VERB  ...     32   39    T\n",
            "1999  dev.en-en.999         click  VERB  ...     32   39    F\n",
            "\n",
            "[2000 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ysGQFh4lKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93c464f-c246-45f8-e26f-875fdb70f393"
      },
      "source": [
        "# test data load chinese\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/datasets/WiC_dataset/norm_wic_dev_new_format.csv\").rename(columns={\"sentence1\":\"sent1\", \"sentence2\":\"sent2\"})\n",
        "\n",
        "# test_df_zr = pd.read_csv('/content/drive/My Drive/datasets/MCL_WiC_old_format/zr+mcl+wic_dev_and_test/zr_test_df.csv')\n",
        "# test_df = test_df_zr[test_df.columns]\n",
        "test_df.head()\n",
        "# test_wic = pd.read_csv('/content/drive/MyDrive/datasets/WiC_dataset/wic_dev_new_format.csv')\n",
        "# test_wic = test_wic[test_df.columns]\n",
        "# test_df =  pd.concat([test_df, test_wic], ignore_index=True)\n",
        "print(test_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           lemma pos  ...                                              sent2  tag\n",
            "0          board   N  ...               He nailed boards across the windows.    F\n",
            "1      circulate   V  ...  This letter is being circulated among the facu...    F\n",
            "2           hook   V  ...  He hooked a snake accidentally, and was so sca...    T\n",
            "3     recreation   N  ...  Drug abuse is often regarded as a form of recr...    T\n",
            "4    domesticity   N  ...  A royal family living in unpretentious domesti...    F\n",
            "..           ...  ..  ...                                                ...  ...\n",
            "633         base   N  ...  Bases include oxides and hydroxides of metals ...    F\n",
            "634        power   N  ...          The mysterious presence of an evil power.    F\n",
            "635  portmanteau   N  ...  ` motel ' is a portmanteau word made by combin...    T\n",
            "636      promise   V  ...                       I promised somebody my time.    T\n",
            "637       pierce   V  ...                       The path pierced the jungle.    F\n",
            "\n",
            "[638 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzddbQYWBSRC"
      },
      "source": [
        "wdf = pd.concat([test_df,train_wic_df])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKzeUzVKQQBE",
        "outputId": "e3a57201-75c8-4354-bf91-cdcf4942bcf0"
      },
      "source": [
        "test_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(638, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6oExRDcVQgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe4c32d-cc4a-47d3-dea3-2a8317599718"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan 28 18:06:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmqHUdDlWC8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c25135-156e-4167-b1d1-a11bcd4b72da"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxokj4qqWRRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ffc162-80c9-4d01-aca2-c70c1a9d2b4a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8MB 14.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=ae8a1f443a0f4b622ff66fdc18e3ae0c6ca1f784fcd7e142095355ee674716ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5IQYV8QWVVm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "cbd3ad0565a14343b858ff6348811864",
            "9913d7a313a241448c88790afe5657b5",
            "1e54bbf5f74b4504b3eccdd40f8c3984",
            "dbed5f3301734dd6ab50af2873e16f9a",
            "1dbd02ba10e24e0682d3bda11f2bddbc",
            "897ab20aced64ecda0784fbe3260b1a5",
            "79b74de8a78a4f44802e3d12a99279fd",
            "1ee36abd25534ef7944d9dcdf5a886c4",
            "c19e119dd25b43b9a4343a12857cf24f",
            "4e79ef91420e402da1b29e5e56a0d0b7",
            "ff598df472704d5cbd4f6d772d6f5569",
            "edc94b8be7f5423688077097bf038b2e",
            "74bfc31ddc3746ac8b734fc1ddd2e221",
            "a5240b34365444a18ea2bf4a1acadc5e",
            "0679678e75a04d028e220978b66c0797",
            "2f64abf9e2bc412b8f69dc2908f15032"
          ]
        },
        "outputId": "6d4bff45-04b6-4085-c541-93e8a9ee6fd0"
      },
      "source": [
        "from transformers import ElectraTokenizerFast\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading ernie tokenizer...')\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-large-discriminator\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading ernie tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbd3ad0565a14343b858ff6348811864",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c19e119dd25b43b9a4343a12857cf24f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7drxaVtnOnat"
      },
      "source": [
        "# sentences is a list 0f str\r\n",
        "\r\n",
        "# Adding a signal without spaces\r\n",
        "train_sentences_1 =  []\r\n",
        "train_sentences_2 = []\r\n",
        "\r\n",
        "dev_sentences_1 =  []\r\n",
        "dev_sentences_2 = []\r\n",
        "\r\n",
        "test_sentences_1 = []\r\n",
        "test_sentences_2 = []\r\n",
        "\r\n",
        "list_train_sentences_1 = list(train_df['sent1'])\r\n",
        "list_train_sentences_2 = list(train_df['sent2'])\r\n",
        "\r\n",
        "list_dev_sentences_1 =  list(dev_df['sent1'])\r\n",
        "list_dev_sentences_2 = list(dev_df['sent2'])\r\n",
        "\r\n",
        "list_test_sentences_1 =  list(test_df['sent1'])\r\n",
        "list_test_sentences_2 = list(test_df['sent2'])\r\n",
        "\r\n",
        "for i in range(len(list_train_sentences_1)):\r\n",
        "  sentence_1 = list_train_sentences_1[i]\r\n",
        "  # print(sentence_1)\r\n",
        "  try:\r\n",
        "    s1 = int(train_df['start1'][i])\r\n",
        "  except:\r\n",
        "    print(train_df['start1'][i])\r\n",
        "    hvl\r\n",
        "  e1 = int(train_df['end1'][i])\r\n",
        "  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\r\n",
        "  train_sentences_1.append(sentence_1)\r\n",
        "\r\n",
        "  sentence_2 = list_train_sentences_2[i]\r\n",
        "  # print(sentence_2)\r\n",
        "  s2 = int(train_df['start2'][i])\r\n",
        "  e2 = int(train_df['end2'][i])\r\n",
        "  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\r\n",
        "  train_sentences_2.append(sentence_2)\r\n",
        "  # print((train_sentences_1[i], train_sentences_2[i]))\r\n",
        "\r\n",
        "for i in range(len(list_dev_sentences_1)):\r\n",
        "  d_sentence_1 = list_dev_sentences_1[i]\r\n",
        "  # print(dev_data_df['start1'][i])\r\n",
        "  s1 = int(dev_df['start1'][i])\r\n",
        "  e1 = int(dev_df['end1'][i])\r\n",
        "  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\r\n",
        "  dev_sentences_1.append(d_sentence_1)\r\n",
        "\r\n",
        "  d_sentence_2 = list_dev_sentences_2[i]\r\n",
        "  s2 = int(dev_df['start2'][i])\r\n",
        "  e2 = int(dev_df['end2'][i])\r\n",
        "  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\r\n",
        "  dev_sentences_2.append(d_sentence_2)\r\n",
        "\r\n",
        "for i in range(len(list_test_sentences_1)):\r\n",
        "  d_sentence_1 = list_test_sentences_1[i]\r\n",
        "  s1 = int(test_df['start1'][i])\r\n",
        "  e1 = int(test_df['end1'][i])\r\n",
        "  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\r\n",
        "  test_sentences_1.append(d_sentence_1)\r\n",
        "\r\n",
        "  d_sentence_2 = list_test_sentences_2[i]\r\n",
        "  s2 = int(test_df['start2'][i])\r\n",
        "  e2 = int(test_df['end2'][i])\r\n",
        "  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\r\n",
        "  test_sentences_2.append(d_sentence_2)\r\n",
        "\r\n",
        "  # print((dev_sentences_1[i], dev_sentences_2[i]))\r\n",
        "\r\n",
        "print((train_sentences_2[:10]))\r\n",
        "print((train_sentences_1[:10]))\r\n",
        "print(dev_sentences_1[:10])\r\n",
        "print(dev_sentences_2[:10])\r\n",
        "print(test_sentences_1[:10])\r\n",
        "print(test_sentences_2[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWsRoatsCm_t"
      },
      "source": [
        "lenths = []\r\n",
        "for i in range(len(train_sentences_1)):\r\n",
        "  e_inp = tokenizer(train_sentences_1[i:i+1], train_sentences_2[i:i+1], padding = True, truncation = True)\r\n",
        "  l = len(e_inp['input_ids'][0])\r\n",
        "  lenths.append(l)\r\n",
        "  # print(e_inp['input_ids'])\r\n",
        "  # kjv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN6jIIpgDeaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357bb0df-32c4-4260-8e48-d57a1a5a3b6a"
      },
      "source": [
        "print(len(lenths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "3MlYpWyuEIms",
        "outputId": "46ac559d-135e-4144-d287-d11ccbf4a602"
      },
      "source": [
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "data = np.array(lenths)\r\n",
        "# fixed bin size\r\n",
        "bins = np.arange(0, 600, 20) # fixed bin size\r\n",
        "\r\n",
        "plt.xlim([min(data)-5, max(data)+5])\r\n",
        "\r\n",
        "plt.hist(data, bins=bins, alpha=0.5)\r\n",
        "plt.title('Random Gaussian data (fixed bin size)')\r\n",
        "plt.xlabel('variable X (bin size = 5)')\r\n",
        "plt.ylabel('count')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtKbGUgDFBMW",
        "outputId": "46ae1d78-da6c-4cde-91e2-f2bf3901fb60"
      },
      "source": [
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMKh6W8wElgz",
        "outputId": "df489131-17ff-45e7-9adc-de3ad20ff927"
      },
      "source": [
        "(data>160).astype(int).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PHvS1LvFulM"
      },
      "source": [
        "train_df = train_df.iloc[data<=160,:].reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tfZcaM6lRlG",
        "outputId": "2ffc58ec-9e8f-4294-e1e5-4e97c8a7d148"
      },
      "source": [
        "train_df['tag'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "F    13463\n",
              "T    13456\n",
              "Name: tag, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKogR69WW9im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e830d19-9690-466d-fa7b-4f323bb8502e"
      },
      "source": [
        "# sentences is a list 0f str\n",
        "\n",
        "# Adding a signal without spaces\n",
        "train_sentences_1 =  []\n",
        "train_sentences_2 = []\n",
        "\n",
        "dev_sentences_1 =  []\n",
        "dev_sentences_2 = []\n",
        "\n",
        "test_sentences_1 = []\n",
        "test_sentences_2 = []\n",
        "\n",
        "list_train_sentences_1 = list(train_df['sent1'])\n",
        "list_train_sentences_2 = list(train_df['sent2'])\n",
        "\n",
        "list_dev_sentences_1 =  list(dev_df['sent1'])\n",
        "list_dev_sentences_2 = list(dev_df['sent2'])\n",
        "\n",
        "list_test_sentences_1 =  list(test_df['sent1'])\n",
        "list_test_sentences_2 = list(test_df['sent2'])\n",
        "\n",
        "for i in range(len(list_train_sentences_1)):\n",
        "  sentence_1 = list_train_sentences_1[i]\n",
        "  # print(sentence_1)\n",
        "  try:\n",
        "    s1 = int(train_df['start1'][i])\n",
        "  except:\n",
        "    print(train_df['start1'][i])\n",
        "    hvl\n",
        "  e1 = int(train_df['end1'][i])\n",
        "  sentence_1 = sentence_1[:s1]+'\"'+sentence_1[s1:e1]+'\"'+ sentence_1[e1:]\n",
        "  train_sentences_1.append(sentence_1)\n",
        "\n",
        "  sentence_2 = list_train_sentences_2[i]\n",
        "  # print(sentence_2)\n",
        "  s2 = int(train_df['start2'][i])\n",
        "  e2 = int(train_df['end2'][i])\n",
        "  sentence_2 = sentence_2[:s2]+'\"'+sentence_2[s2:e2]+'\"'+ sentence_2[e2:]\n",
        "  train_sentences_2.append(sentence_2)\n",
        "  # print((train_sentences_1[i], train_sentences_2[i]))\n",
        "\n",
        "for i in range(len(list_dev_sentences_1)):\n",
        "  d_sentence_1 = list_dev_sentences_1[i]\n",
        "  # print(dev_data_df['start1'][i])\n",
        "  s1 = int(dev_df['start1'][i])\n",
        "  e1 = int(dev_df['end1'][i])\n",
        "  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n",
        "  dev_sentences_1.append(d_sentence_1)\n",
        "\n",
        "  d_sentence_2 = list_dev_sentences_2[i]\n",
        "  s2 = int(dev_df['start2'][i])\n",
        "  e2 = int(dev_df['end2'][i])\n",
        "  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n",
        "  dev_sentences_2.append(d_sentence_2)\n",
        "\n",
        "for i in range(len(list_test_sentences_1)):\n",
        "  d_sentence_1 = list_test_sentences_1[i]\n",
        "  s1 = int(test_df['start1'][i])\n",
        "  e1 = int(test_df['end1'][i])\n",
        "  d_sentence_1 = d_sentence_1[:s1]+'\"'+d_sentence_1[s1:e1]+'\"'+ d_sentence_1[e1:]\n",
        "  test_sentences_1.append(d_sentence_1)\n",
        "\n",
        "  d_sentence_2 = list_test_sentences_2[i]\n",
        "  s2 = int(test_df['start2'][i])\n",
        "  e2 = int(test_df['end2'][i])\n",
        "  d_sentence_2 = d_sentence_2[:s2]+'\"'+d_sentence_2[s2:e2]+'\"'+ d_sentence_2[e2:]\n",
        "  test_sentences_2.append(d_sentence_2)\n",
        "\n",
        "  # print((dev_sentences_1[i], dev_sentences_2[i]))\n",
        "\n",
        "print((train_sentences_2[:10]))\n",
        "print((train_sentences_1[:10]))\n",
        "print(dev_sentences_1[:10])\n",
        "print(dev_sentences_2[:10])\n",
        "print(test_sentences_1[:10])\n",
        "print(test_sentences_2[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A musical \"play\" on the same subject was also staged in Kathmandu for three days.', 'In schools, when water is needed, it is girls who are sent to fetch it, taking time away from their studies and \"play\".', 'Father Lini said that, because of that, the United Nations has a very special place in the affections and \"esteem\" of the people of Vanuatu.', 'This attests to the \"esteem\" and trust enjoyed by your country.', 'A person may be either the \"holder\" of an option, being the person entitled to buy or sell; or the writer of the option, being the person required to honour the holderâ€™s right to buy or sell.', 'Over 5,000 now hold legal immigrant documents, which, after five years of annual renewal, entitles the \"holder\" to apply for permanent residence.', 'The conclusion of the first \"reading\" would make it possible to begin negotiations on a streamlined text.', 'It was precisely that consideration which led the Commission on first \"reading\" to introduce a special regime of compulsory arbitration in cases where countermeasures have been taken.', 'UNDP defines poverty as \"human poverty\", deprivation in the most basic choices people have -- such as to live a long and \"healthy\" life, to be educated, to have the means to a decent standard of living and to be able to be a part of the life of a community.', 'The approach was to promote a \"healthy\" drug-free lifestyle, which should lead to a decline in consumption.']\n",
            "['In that context of coordination and integration, Bolivia holds a key \"play\" in any process of infrastructure development.', 'In that context of coordination and integration, Bolivia holds a key \"play\" in any process of infrastructure development.', 'We would also like to convey our \"esteem\" and congratulations to fraternal Lebanon and its people on the unconditional liberation of its southern part.', 'We would also like to convey our \"esteem\" and congratulations to fraternal Lebanon and its people on the unconditional liberation of its southern part.', 'This growth is the direct result of the increased number of baccalaureate \"holders\", who form the potential market for higher education.', 'This growth is the direct result of the increased number of baccalaureate \"holders\", who form the potential market for higher education.', 'The Units have recreation, including television, \"reading\" and exercise facilities, and arrangements for medical care.', 'The Units have recreation, including television, \"reading\" and exercise facilities, and arrangements for medical care.', 'Yet, protection of the rights of the child called for a \"healthy\" socio-economic environment, financial stability at the national level, sustained economic recovery after financial crises, and international cooperation.', 'Yet, protection of the rights of the child called for a \"healthy\" socio-economic environment, financial stability at the national level, sustained economic recovery after financial crises, and international cooperation.']\n",
            "['No clause in a contract shall be interpreted as evading the responsibility of \"superiors\" under international law.', 'No clause in a contract shall be interpreted as evading the responsibility of \"superiors\" under international law.', 'Such \"acquaintance\" is a right and not an obligation for an accused.', 'Such \"acquaintance\" is a right and not an obligation for an accused.', 'Where any \"baggage\" of any passenger contains firearms or any other prohibited goods these persons are turned over to the police for prosecution.', 'Where any \"baggage\" of any passenger contains firearms or any other prohibited goods these persons are turned over to the police for prosecution.', 'The State party considers it \"improbable\" that the Libyan authorities would take so long to act on his refusal to obey them.', 'The State party considers it \"improbable\" that the Libyan authorities would take so long to act on his refusal to obey them.', 'As to the author\\'s interest in being \"personally\" present at the court hearing, the State party begins by recalling the history of the author\\'s case.', 'As to the author\\'s interest in being \"personally\" present at the court hearing, the State party begins by recalling the history of the author\\'s case.']\n",
            "['While fully aware that bishops and major \"superiors\" of religious institutes do not act as representatives or delegates of the Roman Pontiff, the Committee notes that subordinates in Catholic religious orders are bound by obedience to the Pope, in accordance with Canons 331 and 590 of the Code of canon Law.', 'In Senegal too, the customs officer and his \"superiors\" receive a premium in case of detecting and preventing smuggling.', 'The complaints tend to be lodged against \"acquaintances\", who may be in the individualâ€™s immediate entourage.', 'Sexual violence by non-partners refers to violence by a relative, friend, \"acquaintance\", neighbour, work colleague or stranger.', 'In my \"baggage\" I had a Hungarian grammar book and dictionaries but the police did not allow me to study Hungarian.', 'Moreover, an individual airline may decline to board diplomats or family members who decline to voluntarily permit the search of their personal \"baggage\" or person.', 'However, upon learning that the Swedish authorities considered this proposition \"improbable\", he stated that he participated, but did not arrange, the demonstration.', 'Given the nature of the conflict in Somalia, it is highly \"improbable\" that this ammunition had been imported before the imposition of the arms embargo.', 'The acts performed \"personally\" by an official, however highly placed, who makes use of the State to commit an international crime, can not embrace all the acts of the State constituting that crime.', 'His election to that high post is a tribute not only to him \"personally\", but also to his great and beautiful country.']\n",
            "['Room and \"board\".', '\"Circulate\" a rumor.', '\"Hook\" a fish.', 'For \"recreation\" he wrote poetry and solved crossword puzzles.', 'Making a hobby of \"domesticity\".', 'The child \\'s \"acquisition\" of language.', 'There was no \"meeting\" of minds.', 'They swam in the \"nude\".', 'He left an indelible \"mark\" on the American theater.', 'Conditioning is a form of learning by \"association\".']\n",
            "['He nailed \"boards\" across the windows.', 'This letter is being \"circulated\" among the faculty.', 'He \"hooked\" a snake accidentally, and was so scared he dropped his rod into the water.', 'Drug abuse is often regarded as a form of \"recreation\".', 'A royal family living in unpretentious \"domesticity\".', 'That graphite tennis racquet is quite an \"acquisition\".', 'The \"meeting\" elected a chairperson.', 'The marketing rule \\' \"nude\" sells \\' spread from verbal to visual mainstream media in the 20th century.', 'It was in London that he made his \"mark\".', 'Many close \"associations\" with England.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNh3KxV_qC5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9b90a6-427c-49ab-f250-27a508d99940"
      },
      "source": [
        "encoded_inputs_train = tokenizer(train_sentences_1, train_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n",
        "print(len(encoded_inputs_train['input_ids'][0]))\n",
        "print(type(encoded_inputs_train['input_ids']))\n",
        "\n",
        "encoded_inputs_train_rev = tokenizer(train_sentences_2, train_sentences_1, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n",
        "\n",
        "\n",
        "encoded_inputs_dev = tokenizer(dev_sentences_1, dev_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n",
        "print(len(encoded_inputs_dev['input_ids'][0]))\n",
        "print(type(encoded_inputs_dev['input_ids']))\n",
        "\n",
        "encoded_inputs_test = tokenizer(test_sentences_1, test_sentences_2, padding = True, truncation = True, return_tensors = 'pt', return_offsets_mapping=True)\n",
        "\n",
        "print(test_sentences_1[2])\n",
        "print(test_sentences_2[2])\n",
        "print((encoded_inputs_test['offset_mapping'][2]))\n",
        "print((encoded_inputs_test['input_ids'][2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "160\n",
            "<class 'torch.Tensor'>\n",
            "133\n",
            "<class 'torch.Tensor'>\n",
            "\"Hook\" a fish.\n",
            "He \"hooked\" a snake accidentally, and was so scared he dropped his rod into the water.\n",
            "tensor([[ 0,  0],\n",
            "        [ 0,  1],\n",
            "        [ 1,  5],\n",
            "        [ 5,  6],\n",
            "        [ 7,  8],\n",
            "        [ 9, 13],\n",
            "        [13, 14],\n",
            "        [ 0,  0],\n",
            "        [ 0,  2],\n",
            "        [ 3,  4],\n",
            "        [ 4, 10],\n",
            "        [10, 11],\n",
            "        [12, 13],\n",
            "        [14, 19],\n",
            "        [20, 32],\n",
            "        [32, 33],\n",
            "        [34, 37],\n",
            "        [38, 41],\n",
            "        [42, 44],\n",
            "        [45, 51],\n",
            "        [52, 54],\n",
            "        [55, 62],\n",
            "        [63, 66],\n",
            "        [67, 70],\n",
            "        [71, 75],\n",
            "        [76, 79],\n",
            "        [80, 85],\n",
            "        [85, 86],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0]])\n",
            "tensor([  101,  1000,  8103,  1000,  1037,  3869,  1012,   102,  2002,  1000,\n",
            "        13322,  1000,  1037,  7488,  9554,  1010,  1998,  2001,  2061,  6015,\n",
            "         2002,  3333,  2010,  8473,  2046,  1996,  2300,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnHVEx1ZDzVL"
      },
      "source": [
        "def check_end(offsets, i, e):\r\n",
        "  d = offsets[i][0]\r\n",
        "\r\n",
        "  for j in range(i, len(offsets)):\r\n",
        "    a = offsets[j][0]\r\n",
        "    b = offsets[j][1]\r\n",
        "\r\n",
        "    if (b==e):\r\n",
        "      return True\r\n",
        "    \r\n",
        "    if (a == d):\r\n",
        "      d = b\r\n",
        "      continue\r\n",
        "    else:\r\n",
        "      return False\r\n",
        "  return False\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83_jD71Y72Xb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b597e9a4-1e3c-4183-8de7-4d10bb9da299"
      },
      "source": [
        "# create wordpeice indices of the words of interest now\n",
        "# be aware, that due to signal the actual token have their positions offsetted\n",
        "train_pos_1 = []\n",
        "train_pos_2 = []\n",
        "\n",
        "for j in range(len(train_sentences_1)):\n",
        "  offsets = encoded_inputs_train['offset_mapping'][j].detach().numpy()\n",
        "  second_sent = False\n",
        "  s1 = int(train_df['start1'][j])\n",
        "  s2 = int(train_df['start2'][j])\n",
        "  e1 = int(train_df['end1'][j])\n",
        "  e2 = int(train_df['end2'][j])\n",
        "  pos1 = -1\n",
        "  pos2 = -1\n",
        "  for i, offset in enumerate(offsets):\n",
        "    if i == 0:\n",
        "      continue #cls\n",
        "    if offset[1] == 0:\n",
        "      second_sent = True\n",
        "\n",
        "    if offset[0] == s1+1 and second_sent == False:\n",
        "      if check_end(offsets, i, e1+1):\n",
        "        pos1 = i\n",
        "    elif offset[0] == s2+1 and second_sent == True:\n",
        "      if check_end(offsets, i, e2+1):\n",
        "        pos2 = i\n",
        "        break\n",
        "  if pos1 == -1 or pos2 ==-1:\n",
        "    second_sent = False\n",
        "    for i, offset in enumerate(offsets):\n",
        "      if i == 0:\n",
        "        continue #cls\n",
        "      if offset[1] == 0:\n",
        "        second_sent = True\n",
        "\n",
        "      if offset[0] == s1 and second_sent == False:\n",
        "        if check_end(offsets, i, e1):\n",
        "          pos1 = i\n",
        "      elif offset[0] == s2 and second_sent == True:\n",
        "        if check_end(offsets, i, e2):\n",
        "          pos2 = i\n",
        "          break\n",
        "  if pos1 == -1 or pos2 ==-1:\n",
        "    second_sent = False\n",
        "    for i, offset in enumerate(offsets):\n",
        "      if i == 0:\n",
        "        continue #cls\n",
        "      if offset[1] == 0:\n",
        "        second_sent = True\n",
        "\n",
        "      if offset[0] == s1-1 and second_sent == False:\n",
        "        if check_end(offsets, i, e1-1):\n",
        "          pos1 = i\n",
        "      elif offset[0] == s2-1 and second_sent == True:\n",
        "        if check_end(offsets, i, e2-1):\n",
        "          pos2 = i\n",
        "          break\n",
        "\n",
        "  # if pos1 == -1 or pos2 ==-1:\n",
        "  #   second_sent = False\n",
        "  #   for i, offset in enumerate(offsets):\n",
        "  #     if i == 0:\n",
        "  #       continue #cls\n",
        "  #     if offset[1] == 0:\n",
        "  #       second_sent = True\n",
        "\n",
        "  #     if offset[0] == s1-2 and second_sent == False:\n",
        "  #       if check_end(offsets, i, e1-2):\n",
        "  #         pos1 = i\n",
        "  #     elif offset[0] == s2-2 and second_sent == True:\n",
        "  #       if check_end(offsets, i, e2-2):\n",
        "  #         pos2 = i\n",
        "  #         break\n",
        "  train_pos_1.append(pos1)\n",
        "  train_pos_2.append(pos2)\n",
        "# print(train_pos_1)\n",
        "# print(train_pos_2)\n",
        "\n",
        "dev_pos_1 = []\n",
        "dev_pos_2 = []\n",
        "\n",
        "for j in range(len(dev_sentences_1)):\n",
        "  offsets = encoded_inputs_dev['offset_mapping'][j].detach().numpy()\n",
        "  second_sent = False\n",
        "  s1 = int(dev_df['start1'][j])\n",
        "  s2 = int(dev_df['start2'][j])\n",
        "  e1 = int(dev_df['end1'][j])\n",
        "  e2 = int(dev_df['end2'][j])\n",
        "  pos1 = -1\n",
        "  pos2 = -1\n",
        "  for i, offset in enumerate(offsets):\n",
        "    if i == 0:\n",
        "      continue #cls\n",
        "    if offset[1] == 0:\n",
        "      second_sent = True\n",
        "\n",
        "    if offset[0] == s1+1 and second_sent == False:\n",
        "      if check_end(offsets, i, e1+1):\n",
        "        pos1 = i\n",
        "    elif offset[0] == s2+1 and second_sent == True:\n",
        "      if check_end(offsets, i, e2+1):\n",
        "        pos2 = i\n",
        "        break\n",
        "  if pos1 == -1 or pos2 ==-1:\n",
        "    second_sent = False\n",
        "    for i, offset in enumerate(offsets):\n",
        "      if i == 0:\n",
        "        continue #cls\n",
        "      if offset[1] == 0:\n",
        "        second_sent = True\n",
        "\n",
        "      if offset[0] == s1 and second_sent == False:\n",
        "        if check_end(offsets, i, e1):\n",
        "          pos1 = i\n",
        "      elif offset[0] == s2 and second_sent == True:\n",
        "        if check_end(offsets, i, e2):\n",
        "          pos2 = i\n",
        "          break\n",
        "  if pos1 == -1 or pos2 ==-1:\n",
        "    second_sent = False\n",
        "    for i, offset in enumerate(offsets):\n",
        "      if i == 0:\n",
        "        continue #cls\n",
        "      if offset[1] == 0:\n",
        "        second_sent = True\n",
        "\n",
        "      if offset[0] == s1-1 and second_sent == False:\n",
        "        if check_end(offsets, i, e1-1):\n",
        "          pos1 = i\n",
        "      elif offset[0] == s2-1 and second_sent == True:\n",
        "        if check_end(offsets, i, e2-1):\n",
        "          pos2 = i\n",
        "          break\n",
        "\n",
        "\n",
        "  dev_pos_1.append(pos1)\n",
        "  dev_pos_2.append(pos2)\n",
        "\n",
        "test_pos_1 = []\n",
        "test_pos_2 = []\n",
        "\n",
        "for j in range(len(test_sentences_1)):\n",
        "  offsets = encoded_inputs_test['offset_mapping'][j].detach().numpy()\n",
        "  second_sent = False\n",
        "  s1 = int(test_df['start1'][j])\n",
        "  s2 = int(test_df['start2'][j])\n",
        "  e1 = int(test_df['end1'][j])\n",
        "  e2 = int(test_df['end2'][j])\n",
        "  pos1 = -1\n",
        "  pos2 = -1\n",
        "  for i, offset in enumerate(offsets):\n",
        "    if i == 0:\n",
        "      continue #cls\n",
        "    if offset[1] == 0:\n",
        "      second_sent = True\n",
        "\n",
        "    if offset[0] == s1+1 and second_sent == False:\n",
        "      if check_end(offsets, i, e1+1):\n",
        "        pos1 = i\n",
        "    elif offset[0] == s2+1 and second_sent == True:\n",
        "      if check_end(offsets, i, e2+1):\n",
        "        pos2 = i\n",
        "        break\n",
        "  if pos1 == -1 or pos2 ==-1:\n",
        "    second_sent = False\n",
        "    for i, offset in enumerate(offsets):\n",
        "      if i == 0:\n",
        "        continue #cls\n",
        "      if offset[1] == 0:\n",
        "        second_sent = True\n",
        "\n",
        "      if offset[0] == s1 and second_sent == False:\n",
        "        if check_end(offsets, i, e1):\n",
        "          pos1 = i\n",
        "      elif offset[0] == s2 and second_sent == True:\n",
        "        if check_end(offsets, i, e2):\n",
        "          pos2 = i\n",
        "          break\n",
        "  if pos1 == -1 or pos2 ==-1:\n",
        "    second_sent = False\n",
        "    for i, offset in enumerate(offsets):\n",
        "      if i == 0:\n",
        "        continue #cls\n",
        "      if offset[1] == 0:\n",
        "        second_sent = True\n",
        "\n",
        "      if offset[0] == s1-1 and second_sent == False:\n",
        "        if check_end(offsets, i, e1-1):\n",
        "          pos1 = i\n",
        "      elif offset[0] == s2-1 and second_sent == True:\n",
        "        if check_end(offsets, i, e2-1):\n",
        "          pos2 = i\n",
        "          break\n",
        "  test_pos_1.append(pos1)\n",
        "  test_pos_2.append(pos2)\n",
        "print(dev_pos_1[:100])\n",
        "print(dev_pos_2[800:810])\n",
        "print(test_pos_1[:10])\n",
        "print(len(test_pos_2))\n",
        "\n",
        "train_pos_1 = torch.LongTensor(train_pos_1)\n",
        "train_pos_2 = torch.LongTensor(train_pos_2)\n",
        "\n",
        "train_pos = torch.stack((train_pos_1, train_pos_2), dim =1)\n",
        "\n",
        "dev_pos_1 = torch.LongTensor(dev_pos_1)\n",
        "dev_pos_2 = torch.LongTensor(dev_pos_2)\n",
        "\n",
        "dev_pos = torch.stack((dev_pos_1, dev_pos_2), dim=1)\n",
        "\n",
        "test_pos_1 = torch.LongTensor(test_pos_1)\n",
        "test_pos_2 = torch.LongTensor(test_pos_2)\n",
        "\n",
        "test_pos = torch.stack((test_pos_1, test_pos_2), dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16, 16, 3, 3, 4, 4, 7, 7, 11, 11, 13, 13, 9, 9, 17, 17, 8, 8, 9, 9, 8, 8, 7, 7, 8, 8, 18, 18, 13, 13, 40, 40, 21, 21, 6, 6, 25, 25, 62, 62, 18, 18, 10, 10, 10, 10, 9, 9, 11, 11, 3, 3, 13, 13, 6, 6, 9, 9, 7, 7, 12, 12, 6, 6, 11, 11, 20, 20, 29, 29, 27, 27, 35, 35, 14, 14, 15, 15, 10, 10, 7, 7, 17, 17, 6, 6, 3, 3, 3, 3, 23, 23, 15, 15, 7, 7, 11, 11, 15, 15]\n",
            "[55, 51, 35, 59, 35, 52, 43, 40, 39, 45]\n",
            "[4, 2, 2, 3, 6, 6, 5, 6, 8, 9]\n",
            "638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ5euboiEW7L",
        "outputId": "14f0911e-4470-4b32-c597-4a1e5d5b2a67"
      },
      "source": [
        "print(tokenizer.encode('Hi there you fool.'))\r\n",
        "print(tokenizer.encode('Hi there \"you\" fool.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 7632, 2045, 2017, 7966, 1012, 102]\n",
            "[101, 7632, 2045, 1000, 2017, 1000, 7966, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bReWCVuYgN2J"
      },
      "source": [
        "# print(train_pos_1[:20])\n",
        "# print(train_pos_2[:20])\n",
        "\n",
        "# print(train_pos_1[8000:8020])\n",
        "# print(train_pos_2[8000:8020])\n",
        "# print(len(train_pos_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLQvrr_Auw_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ceffbe-5781-46a1-f159-1eaf05551896"
      },
      "source": [
        "# labels = torch.from_numpy(train_gold_df['label'].values)\n",
        "train_gold_df_tmp = train_df[['tag']].replace({'F' : 0, 'T' : 1})\n",
        "train_labels = torch.from_numpy(train_gold_df_tmp.values)\n",
        "print(train_labels)\n",
        "\n",
        "dev_gold_df_tmp = dev_df[['tag']].replace({'F' : 0, 'T' : 1})\n",
        "dev_labels = torch.from_numpy(dev_gold_df_tmp.values)\n",
        "\n",
        "test_gold_df_tmp = test_df[['tag']].replace({'F' : 0, 'T' : 1})\n",
        "test_labels = torch.from_numpy(test_gold_df_tmp.values)\n",
        "print(len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0],\n",
            "        [0],\n",
            "        [1],\n",
            "        ...,\n",
            "        [1],\n",
            "        [0],\n",
            "        [1]])\n",
            "638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17X2mWJJFuUB"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "train_dataset = TensorDataset(encoded_inputs_train['input_ids'], encoded_inputs_train['token_type_ids'],\n",
        "                              encoded_inputs_train['attention_mask'], train_pos, train_labels)\n",
        "\n",
        "\n",
        "dev_dataset = TensorDataset(encoded_inputs_dev['input_ids'], encoded_inputs_dev['token_type_ids'],\n",
        "                              encoded_inputs_dev['attention_mask'], dev_pos, dev_labels)\n",
        "\n",
        "test_dataset = TensorDataset(encoded_inputs_test['input_ids'], encoded_inputs_test['token_type_ids'],\n",
        "                              encoded_inputs_test['attention_mask'], test_pos, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIOoyHpCGo9U"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "dev_dataloader = DataLoader(\n",
        "            dev_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMHW4hZoFST1"
      },
      "source": [
        "**CUTOFF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odLzbaVGGw0F"
      },
      "source": [
        "from transformers import ElectraModel, AdamW, BertConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "class BERTi(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERTi, self).__init__()\n",
        "\n",
        "        options_name = \"google/electra-large-discriminator\"\n",
        "        hidden_states = False\n",
        "        self.encoder = ElectraModel.from_pretrained(options_name, output_hidden_states = hidden_states, return_dict = False)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "        last_layer = self.encoder(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)[0]\n",
        "\n",
        "        return last_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gfYEvqv-qET"
      },
      "source": [
        "class Logistic_Reg(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Logistic_Reg, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(2048, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BMpmNJmG3QN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "8d8b29ccaacc4b18a88581e426af29ba",
            "38ca88a520cc43f6b313f697cd329329",
            "e3631cb104e74bc1b8d373ccd9455491",
            "8c6857a297824765adb07aa9b6fd8108",
            "f06b975f83db40ba8b02fa71592e74f9",
            "9af27b16d0084d868dcd1c0da00f64e2",
            "96cbeca929524ba2ba07dadad4040cb2",
            "f7b50ea253654ac2a0555150b73c4038",
            "046212b13cd248b1b500b4df1a735e0e",
            "269f332e2bb04f849b09160a24e3f316",
            "7b9818d75c4b42dd881f52020cac0ff8",
            "d83c4c09a1df45e2a066398e056f7765",
            "a0d4ae9002e544679fa74db1402400ed",
            "883739284f8f474e9d14943b1697af68",
            "0e708e87c9024d98832bf22b29246737",
            "ae61160e119841b6a9674a8efa0589bb"
          ]
        },
        "outputId": "82f5b144-4995-465a-edc6-cd9fea57a359"
      },
      "source": [
        "model_bert = BERTi().to(device)\n",
        "model_log_reg = Logistic_Reg().to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d8b29ccaacc4b18a88581e426af29ba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=469.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "046212b13cd248b1b500b4df1a735e0e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344867008.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXtgT0yeykKD"
      },
      "source": [
        "# model_bert.load_state_dict(torch.load(\"/content/drive/MyDrive/Model Analysis/Ausemcor/ernie_large_ausem_small_wic/model_bert_dev_best_dev_X_test_X_mcl_and_wic\"))\r\n",
        "# model_log_reg.load_state_dict(torch.load(\"/content/drive/MyDrive/Model Analysis/Ausemcor/ernie_large_ausem_small_wic/model_log_reg_dev_bestdev_X_test_X_mcl_and_wic\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaxHb5XzHSA9"
      },
      "source": [
        "optimizer = AdamW(list(model_bert.parameters()) + list(model_log_reg.parameters()),\n",
        "                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 1e-5\n",
        "                  eps = 1e-8 ,\n",
        "                  # weight_decay = 1 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# scheduler??\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZMmS3xuw742"
      },
      "source": [
        "def loss_fn(output, targets):\n",
        "  return nn.BCEWithLogitsLoss(reduction='mean')(output, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0jhTg1VHfdR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    print(pred_flat)\n",
        "    print(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGjyERYSd68E"
      },
      "source": [
        "def flat_accuracy_single_logit(preds, labels):\n",
        "    pred_flat = (preds>0).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    # print(pred_flat)\n",
        "    # print(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shJmEVhWQE7g"
      },
      "source": [
        "def flat_accuracy_single_logit_avg(preds, labels, m):\r\n",
        "    pred_flat = (1/(1+np.exp(-preds))).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    # print(pred_flat)\r\n",
        "    # print(labels_flat)\r\n",
        "    return np.sum(((pred_flat[:m]+pred_flat[m:])>1).astype(int) == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTJU8ytyQCCA"
      },
      "source": [
        "def cross_entropy(predictions, targets):\r\n",
        "    N = predictions.shape[0]\r\n",
        "    predictions = 1/(1+np.exp(-predictions))\r\n",
        "    ce = -np.sum(targets * np.log(predictions) + (1-targets) * np.log(1-predictions)) / N\r\n",
        "    return ce\r\n",
        "\r\n",
        "def cross_entropy_avg(predictions, targets,m):\r\n",
        "    N = predictions.shape[0]\r\n",
        "    predictions = 1/(1+np.exp(-predictions))\r\n",
        "    predictions = (predictions[:m]+predictions[m:])/2\r\n",
        "    ce = -np.sum(targets * np.log(predictions) + (1-targets) * np.log(1-predictions)) / N\r\n",
        "    return ce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBd7Pv-yHiv8"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWzs7IuM3wWP"
      },
      "source": [
        "path_this = \"/content/drive/MyDrive/SemEval Models b/English/electra_mcl_rev_wic_t_ausem\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0lnd90FLg_f"
      },
      "source": [
        "accumulation_steps = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0pG6f5CHkqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c7c815d-c73c-4c98-a509-3ff017c2a6d7"
      },
      "source": [
        "# import random\n",
        "# import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "epochs = 12\n",
        "best_val_accuracy_n = 0\n",
        "best_val_accuracy_avg = 0\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model_bert.train()\n",
        "    model_log_reg.train()\n",
        "\n",
        "    if True:\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          # if step == 0:\n",
        "          #   break\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 100 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          if (step == int(len(train_dataloader)/5) or step == int(2*len(train_dataloader)/5) or step == int(3*len(train_dataloader)/5)\n",
        "          or step == int(4*len(train_dataloader)/5)):\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\n",
        "            # during evaluation.\n",
        "            model_bert.eval()\n",
        "            model_log_reg.eval()\n",
        "\n",
        "            # Tracking variables \n",
        "            total_eval_accuracy = 0\n",
        "            total_eval_loss = 0\n",
        "            nb_eval_steps = 0\n",
        "\n",
        "            t_labels = []\n",
        "            t_preds = []\n",
        "\n",
        "            # Evaluate data for one epoch\n",
        "            for atch in dev_dataloader:\n",
        "                \n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "                # the `to` method.\n",
        "\n",
        "                b_input_ids = atch[0].to(device)\n",
        "                b_token_type_ids = atch[1].to(device)\n",
        "                b_attention_mask = atch[2].to(device)\n",
        "                b_poses = atch[3].to(device)\n",
        "                b_labels = atch[4].to(device)\n",
        "                \n",
        "                # Tell pytorch not to bother with constructing the compute graph during\n",
        "                # the forward pass, since this is only needed for backprop (training).\n",
        "                with torch.no_grad():        \n",
        "\n",
        "                    last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n",
        "                    b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n",
        "                    gathered_activations = torch.gather(last_layer, 1, b_poses)\n",
        "                    logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n",
        "                    loss = loss_fn(logits, b_labels.type_as(logits))\n",
        "                    \n",
        "                # Accumulate the validation loss.\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits.cpu().detach().numpy()\n",
        "                # print(logits)\n",
        "                label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "                t_labels.append(label_ids)\n",
        "                t_preds.append(logits)\n",
        "                \n",
        "\n",
        "            all_dev_labels = np.concatenate(t_labels, axis=0)\n",
        "            all_dev_logits = np.concatenate(t_preds, axis=0)\n",
        "            # Report the final accuracy for this validation run.\n",
        "            # print(all_dev_logits)\n",
        "            avg_val_accuracy_n = flat_accuracy_single_logit(all_dev_logits[:1000], all_dev_labels[:1000])\n",
        "            print(\"Normal Validation Accuracy: {0:.4f}\".format(avg_val_accuracy_n))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            # avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "            print(\"   Normal Validation Loss: {0:.3f}\".format(cross_entropy(all_dev_logits[:1000], all_dev_labels[:1000])))\n",
        "\n",
        "            avg_val_accuracy_avg = flat_accuracy_single_logit_avg(all_dev_logits, all_dev_labels[:1000], 1000)\n",
        "            print(\"Rev Avg Validation Accuracy: {0:.4f}\".format(avg_val_accuracy_avg))\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            # avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "            print(\"   Rev Avg Validation Loss: {0:.3f}\".format(cross_entropy_avg(all_dev_logits, all_dev_labels[:1000], 1000)))\n",
        "            \n",
        "            # Measure how long the validation run took.\n",
        "            validation_time = format_time(time.time() - t0)\n",
        "\n",
        "            if(avg_val_accuracy_n > best_val_accuracy_n):\n",
        "              # flag = True\n",
        "              best_val_accuracy_n = avg_val_accuracy_n\n",
        "              best_val_preds = all_dev_logits\n",
        "              np.save(os.path.join(path_this, \"dev_preds_normal\"), best_val_preds)\n",
        "              # best_bert_parameters = model_bert.state_dict()\n",
        "              # best_log_reg_parameters = model_log_reg.state_dict()    \n",
        "              torch.save(model_bert.state_dict(), os.path.join(path_this, \"normal_model_bert_dev_best\"))\n",
        "              torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"normal_model_log_reg_dev_best\"))\n",
        "\n",
        "            if(avg_val_accuracy_avg > best_val_accuracy_avg):\n",
        "              # flag = True\n",
        "              best_val_accuracy_avg = avg_val_accuracy_avg\n",
        "              best_val_preds = all_dev_logits\n",
        "              np.save(os.path.join(path_this, \"rev_avg_dev_preds\"), best_val_preds)\n",
        "              # best_bert_parameters = model_bert.state_dict()\n",
        "              # best_log_reg_parameters = model_log_reg.state_dict()    \n",
        "              torch.save(model_bert.state_dict(), os.path.join(path_this, \"rev_avg_model_bert_dev_best\"))\n",
        "              torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"rev_avg_model_log_reg_dev_best\"))\n",
        "            \n",
        "            # print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "            model_bert.train()\n",
        "            model_log_reg.train()\n",
        "\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_token_type_ids = batch[1].to(device)\n",
        "          b_attention_mask = batch[2].to(device)\n",
        "          b_poses = batch[3].to(device)\n",
        "          b_labels = batch[4].to(device)\n",
        "\n",
        "          if step%accumulation_steps == 0:\n",
        "            optimizer.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          # It returns different numbers of parameters depending on what arguments\n",
        "          # arge given and what flags are set. For our useage here, it returns\n",
        "          # the loss (because we provided labels) and the \"logits\"--the model\n",
        "          # outputs prior to activation.\n",
        "          last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n",
        "\n",
        "          # print(b_poses[:,0].size())\n",
        "          b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n",
        "          # print(b_poses.size())\n",
        "          gathered_activations = torch.gather(last_layer, 1, b_poses)\n",
        "\n",
        "          # concatted = torch.cat((last_layer[b_poses[:,0].view(16,1),:] , last_layer[:,b_poses[:,1],:]), dim = 1)\n",
        "\n",
        "          logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n",
        "          # print(type(b_labels))\n",
        "          loss = loss_fn(logits, b_labels.type_as(logits))\n",
        "          loss = loss/accumulation_steps\n",
        "          # print(\"loss = \"+str(loss))\n",
        "          # print(logits.cpu().detach().numpy())\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          \n",
        "          # print(loss)\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          # TODO logsistic REG????/\n",
        "          if (step%accumulation_steps == (accumulation_steps-1)):\n",
        "            torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "          # # Update the learning rate.\n",
        "          # scheduler.step()\n",
        "        \n",
        "    \n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model_bert.eval()\n",
        "    model_log_reg.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    t_labels = []\n",
        "    t_preds = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in dev_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_type_ids = batch[1].to(device)\n",
        "        b_attention_mask = batch[2].to(device)\n",
        "        b_poses = batch[3].to(device)\n",
        "        b_labels = batch[4].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n",
        "            b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 1024)\n",
        "            gathered_activations = torch.gather(last_layer, 1, b_poses)\n",
        "            logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n",
        "            loss = loss_fn(logits, b_labels.type_as(logits))\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.cpu().detach().numpy()\n",
        "        # print(logits)\n",
        "        label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "        t_labels.append(label_ids)\n",
        "        t_preds.append(logits)\n",
        "        \n",
        "\n",
        "    all_dev_labels = np.concatenate(t_labels, axis=0)\n",
        "    all_dev_logits = np.concatenate(t_preds, axis=0)\n",
        "    # Report the final accuracy for this validation run.\n",
        "    # print(all_dev_logits)\n",
        "    avg_val_accuracy_n = flat_accuracy_single_logit(all_dev_logits[:1000], all_dev_labels[:1000])\n",
        "    print(\"Normal Validation Accuracy: {0:.4f}\".format(avg_val_accuracy_n))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    # avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    print(\"   Normal Validation Loss: {0:.3f}\".format(cross_entropy(all_dev_logits[:1000], all_dev_labels[:1000])))\n",
        "\n",
        "    avg_val_accuracy_avg = flat_accuracy_single_logit_avg(all_dev_logits, all_dev_labels[:1000], 1000)\n",
        "    print(\"Rev Avg Validation Accuracy: {0:.4f}\".format(avg_val_accuracy_avg))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    # avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "    print(\"   Rev Avg Validation Loss: {0:.3f}\".format(cross_entropy_avg(all_dev_logits, all_dev_labels[:1000], 1000)))\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    if(avg_val_accuracy_n > best_val_accuracy_n):\n",
        "      # flag = True\n",
        "      best_val_accuracy_n = avg_val_accuracy_n\n",
        "      best_val_preds = all_dev_logits\n",
        "      np.save(os.path.join(path_this, \"dev_preds_normal\"), best_val_preds)\n",
        "      # best_bert_parameters = model_bert.state_dict()\n",
        "      # best_log_reg_parameters = model_log_reg.state_dict()    \n",
        "      torch.save(model_bert.state_dict(), os.path.join(path_this, \"normal_model_bert_dev_best\"))\n",
        "      torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"normal_model_log_reg_dev_best\"))\n",
        "\n",
        "    if(avg_val_accuracy_avg > best_val_accuracy_avg):\n",
        "      # flag = True\n",
        "      best_val_accuracy_avg = avg_val_accuracy_avg\n",
        "      best_val_preds = all_dev_logits\n",
        "      np.save(os.path.join(path_this, \"rev_avg_dev_preds\"), best_val_preds)\n",
        "      # best_bert_parameters = model_bert.state_dict()\n",
        "      # best_log_reg_parameters = model_log_reg.state_dict()    \n",
        "      torch.save(model_bert.state_dict(), os.path.join(path_this, \"rev_avg_model_bert_dev_best\"))\n",
        "      torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"rev_avg_model_log_reg_dev_best\"))\n",
        "    \n",
        "    # print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    \n",
        "\n",
        "\n",
        "    # # Do same on test data\n",
        "    # # Tracking variables \n",
        "    # total_eval_accuracy = 0\n",
        "    # total_eval_loss = 0\n",
        "    # nb_eval_steps = 0\n",
        "\n",
        "    # t_labels = []\n",
        "    # t_preds = []\n",
        "\n",
        "    # # Evaluate data for one epoch\n",
        "    # for batch in test_dataloader:\n",
        "        \n",
        "    #     # Unpack this training batch from our dataloader. \n",
        "    #     #\n",
        "    #     # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "    #     # the `to` method.\n",
        "\n",
        "    #     b_input_ids = batch[0].to(device)\n",
        "    #     b_token_type_ids = batch[1].to(device)\n",
        "    #     b_attention_mask = batch[2].to(device)\n",
        "    #     b_poses = batch[3].to(device)\n",
        "    #     b_labels = batch[4].to(device)\n",
        "        \n",
        "    #     # Tell pytorch not to bother with constructing the compute graph during\n",
        "    #     # the forward pass, since this is only needed for backprop (training).\n",
        "    #     with torch.no_grad():\n",
        "    #         last_layer = model_bert(b_input_ids, b_token_type_ids, b_attention_mask)\n",
        "    #         b_poses = b_poses.unsqueeze(-1).repeat(1, 1, 4096)\n",
        "    #         gathered_activations = torch.gather(last_layer, 1, b_poses)\n",
        "    #         logits = model_log_reg(gathered_activations.view(gathered_activations.size()[0], -1))\n",
        "    #         loss = loss_fn(logits, b_labels.type_as(logits))\n",
        "            \n",
        "    #     # Accumulate the validation loss.\n",
        "    #     total_eval_loss += loss.item()\n",
        "\n",
        "    #     # Move logits and labels to CPU\n",
        "    #     logits = logits.cpu().detach().numpy()\n",
        "    #     label_ids = b_labels.cpu().numpy()\n",
        "\n",
        "    #     t_labels.append(label_ids)\n",
        "    #     t_preds.append(logits)\n",
        "        \n",
        "\n",
        "    # all_dev_labels = np.concatenate(t_labels, axis=0)\n",
        "    # all_dev_logits = np.concatenate(t_preds, axis=0)\n",
        "    # # Report the final accuracy for this validation run.\n",
        "    # avg_val_accuracy = flat_accuracy_single_logit(all_dev_logits, all_dev_labels)\n",
        "    # print(\"Test Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # if flag == True:\n",
        "    #   np.save(os.path.join(path_this, \"test_preds_at_best_dev\"), best_val_preds)\n",
        "    #   flag = False\n",
        "\n",
        "    # if(avg_val_accuracy > t_best_val_accuracy):\n",
        "    #   t_best_val_accuracy = avg_val_accuracy\n",
        "    #   t_best_val_preds = all_dev_logits\n",
        "    #   np.save(os.path.join(path_this, \"test_preds\"), t_best_val_preds)\n",
        "    #   torch.save(model_bert.state_dict(), os.path.join(path_this, \"model_bert_test_best\"))\n",
        "    #   torch.save(model_log_reg.state_dict(), os.path.join(path_this, \"model_log_reg_test_best\"))\n",
        "\n",
        "    # # Calculate the average loss over all of the batches.\n",
        "    # avg_val_loss = total_eval_loss / len(test_dataloader)\n",
        "    \n",
        "    # # Measure how long the validation run took.\n",
        "    # validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    # print(\"  Test Avg Loss: {0:.3f}\".format(avg_val_loss))\n",
        "    # print(\"  Testing Validation took: {:}\".format(validation_time))\n",
        "\n",
        "   \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:36.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:24.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8610\n",
            "   Normal Validation Loss: 0.354\n",
            "Rev Avg Validation Accuracy: 0.8560\n",
            "   Rev Avg Validation Loss: 0.174\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:01:08.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:56.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:44.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8700\n",
            "   Normal Validation Loss: 0.320\n",
            "Rev Avg Validation Accuracy: 0.8720\n",
            "   Rev Avg Validation Loss: 0.157\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:50.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:39.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:27.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:03:16.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8800\n",
            "   Normal Validation Loss: 0.298\n",
            "Rev Avg Validation Accuracy: 0.8810\n",
            "   Rev Avg Validation Loss: 0.148\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:01:20.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:02:09.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:57.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8920\n",
            "   Normal Validation Loss: 0.313\n",
            "Rev Avg Validation Accuracy: 0.8990\n",
            "   Rev Avg Validation Loss: 0.152\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:01:02.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:51.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:39.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:03:19\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8910\n",
            "   Normal Validation Loss: 0.285\n",
            "Rev Avg Validation Accuracy: 0.8960\n",
            "   Rev Avg Validation Loss: 0.136\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 2 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:36.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:25.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8830\n",
            "   Normal Validation Loss: 0.400\n",
            "Rev Avg Validation Accuracy: 0.8850\n",
            "   Rev Avg Validation Loss: 0.189\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:23.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8940\n",
            "   Normal Validation Loss: 0.414\n",
            "Rev Avg Validation Accuracy: 0.8970\n",
            "   Rev Avg Validation Loss: 0.191\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:36.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:25.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:13.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:03:02.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8970\n",
            "   Normal Validation Loss: 0.366\n",
            "Rev Avg Validation Accuracy: 0.9050\n",
            "   Rev Avg Validation Loss: 0.167\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:01:20.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:02:09.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:57.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8920\n",
            "   Normal Validation Loss: 0.376\n",
            "Rev Avg Validation Accuracy: 0.8980\n",
            "   Rev Avg Validation Loss: 0.172\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:42.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:30.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:02:58\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8860\n",
            "   Normal Validation Loss: 0.421\n",
            "Rev Avg Validation Accuracy: 0.8880\n",
            "   Rev Avg Validation Loss: 0.197\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 3 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:36.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:24.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8930\n",
            "   Normal Validation Loss: 0.446\n",
            "Rev Avg Validation Accuracy: 0.9000\n",
            "   Rev Avg Validation Loss: 0.195\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:34.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:23.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8880\n",
            "   Normal Validation Loss: 0.461\n",
            "Rev Avg Validation Accuracy: 0.8880\n",
            "   Rev Avg Validation Loss: 0.208\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:29.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:17.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:05.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:53.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8860\n",
            "   Normal Validation Loss: 0.461\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.209\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:47.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:36.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8890\n",
            "   Normal Validation Loss: 0.497\n",
            "Rev Avg Validation Accuracy: 0.8900\n",
            "   Rev Avg Validation Loss: 0.225\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:42.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:30.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:02:58\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8960\n",
            "   Normal Validation Loss: 0.464\n",
            "Rev Avg Validation Accuracy: 0.8980\n",
            "   Rev Avg Validation Loss: 0.202\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 4 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:36.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:24.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8960\n",
            "   Normal Validation Loss: 0.510\n",
            "Rev Avg Validation Accuracy: 0.8930\n",
            "   Rev Avg Validation Loss: 0.235\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:34.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:23.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8860\n",
            "   Normal Validation Loss: 0.579\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.258\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:29.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:17.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:05.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:53.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8940\n",
            "   Normal Validation Loss: 0.497\n",
            "Rev Avg Validation Accuracy: 0.8980\n",
            "   Rev Avg Validation Loss: 0.219\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:48.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:36.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8880\n",
            "   Normal Validation Loss: 0.523\n",
            "Rev Avg Validation Accuracy: 0.8940\n",
            "   Rev Avg Validation Loss: 0.243\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:30.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:02:58\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8890\n",
            "   Normal Validation Loss: 0.497\n",
            "Rev Avg Validation Accuracy: 0.8920\n",
            "   Rev Avg Validation Loss: 0.221\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 5 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:36.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:24.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8880\n",
            "   Normal Validation Loss: 0.543\n",
            "Rev Avg Validation Accuracy: 0.8930\n",
            "   Rev Avg Validation Loss: 0.224\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:34.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:22.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8890\n",
            "   Normal Validation Loss: 0.543\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.255\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:04.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:52.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8850\n",
            "   Normal Validation Loss: 0.567\n",
            "Rev Avg Validation Accuracy: 0.8840\n",
            "   Rev Avg Validation Loss: 0.245\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:47.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:35.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8890\n",
            "   Normal Validation Loss: 0.545\n",
            "Rev Avg Validation Accuracy: 0.8910\n",
            "   Rev Avg Validation Loss: 0.231\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:29.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:17.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:02:57\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8920\n",
            "   Normal Validation Loss: 0.471\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.203\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 6 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:23.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8850\n",
            "   Normal Validation Loss: 0.522\n",
            "Rev Avg Validation Accuracy: 0.8940\n",
            "   Rev Avg Validation Loss: 0.227\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:34.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:21.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8880\n",
            "   Normal Validation Loss: 0.526\n",
            "Rev Avg Validation Accuracy: 0.8920\n",
            "   Rev Avg Validation Loss: 0.214\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:04.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:51.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8810\n",
            "   Normal Validation Loss: 0.539\n",
            "Rev Avg Validation Accuracy: 0.8860\n",
            "   Rev Avg Validation Loss: 0.226\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:47.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:34.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8830\n",
            "   Normal Validation Loss: 0.525\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.222\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:29.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:17.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:02:56\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8840\n",
            "   Normal Validation Loss: 0.496\n",
            "Rev Avg Validation Accuracy: 0.8810\n",
            "   Rev Avg Validation Loss: 0.231\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 7 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:23.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8740\n",
            "   Normal Validation Loss: 0.593\n",
            "Rev Avg Validation Accuracy: 0.8770\n",
            "   Rev Avg Validation Loss: 0.248\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:34.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:21.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8800\n",
            "   Normal Validation Loss: 0.564\n",
            "Rev Avg Validation Accuracy: 0.8810\n",
            "   Rev Avg Validation Loss: 0.244\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:04.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:51.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8770\n",
            "   Normal Validation Loss: 0.567\n",
            "Rev Avg Validation Accuracy: 0.8750\n",
            "   Rev Avg Validation Loss: 0.245\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:47.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:34.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8690\n",
            "   Normal Validation Loss: 0.606\n",
            "Rev Avg Validation Accuracy: 0.8790\n",
            "   Rev Avg Validation Loss: 0.265\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:29.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:16.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:02:56\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8820\n",
            "   Normal Validation Loss: 0.549\n",
            "Rev Avg Validation Accuracy: 0.8830\n",
            "   Rev Avg Validation Loss: 0.246\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 8 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:47.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:22.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8860\n",
            "   Normal Validation Loss: 0.539\n",
            "Rev Avg Validation Accuracy: 0.8940\n",
            "   Rev Avg Validation Loss: 0.232\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:33.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:21.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8710\n",
            "   Normal Validation Loss: 0.639\n",
            "Rev Avg Validation Accuracy: 0.8760\n",
            "   Rev Avg Validation Loss: 0.266\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:03.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:51.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8900\n",
            "   Normal Validation Loss: 0.530\n",
            "Rev Avg Validation Accuracy: 0.8950\n",
            "   Rev Avg Validation Loss: 0.222\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:46.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:34.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8850\n",
            "   Normal Validation Loss: 0.549\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.221\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:29.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:16.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:02:56\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8790\n",
            "   Normal Validation Loss: 0.593\n",
            "Rev Avg Validation Accuracy: 0.8780\n",
            "   Rev Avg Validation Loss: 0.255\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 9 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:48.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:22.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8800\n",
            "   Normal Validation Loss: 0.594\n",
            "Rev Avg Validation Accuracy: 0.8900\n",
            "   Rev Avg Validation Loss: 0.260\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:33.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:21.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8840\n",
            "   Normal Validation Loss: 0.563\n",
            "Rev Avg Validation Accuracy: 0.8950\n",
            "   Rev Avg Validation Loss: 0.245\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:03.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:51.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8940\n",
            "   Normal Validation Loss: 0.512\n",
            "Rev Avg Validation Accuracy: 0.8970\n",
            "   Rev Avg Validation Loss: 0.215\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:46.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:33.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8710\n",
            "   Normal Validation Loss: 0.627\n",
            "Rev Avg Validation Accuracy: 0.8830\n",
            "   Rev Avg Validation Loss: 0.251\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:29.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:16.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:02:55\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8860\n",
            "   Normal Validation Loss: 0.548\n",
            "Rev Avg Validation Accuracy: 0.8950\n",
            "   Rev Avg Validation Loss: 0.239\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 10 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:47.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:22.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8880\n",
            "   Normal Validation Loss: 0.574\n",
            "Rev Avg Validation Accuracy: 0.8850\n",
            "   Rev Avg Validation Loss: 0.261\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:33.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:20.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8830\n",
            "   Normal Validation Loss: 0.613\n",
            "Rev Avg Validation Accuracy: 0.8880\n",
            "   Rev Avg Validation Loss: 0.254\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:03.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:50.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8890\n",
            "   Normal Validation Loss: 0.591\n",
            "Rev Avg Validation Accuracy: 0.8930\n",
            "   Rev Avg Validation Loss: 0.248\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:46.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:33.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8860\n",
            "   Normal Validation Loss: 0.577\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.240\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:29.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:16.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:02:55\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8730\n",
            "   Normal Validation Loss: 0.658\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.260\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 11 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:47.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:22.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8840\n",
            "   Normal Validation Loss: 0.601\n",
            "Rev Avg Validation Accuracy: 0.8970\n",
            "   Rev Avg Validation Loss: 0.250\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:33.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:20.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8870\n",
            "   Normal Validation Loss: 0.613\n",
            "Rev Avg Validation Accuracy: 0.8810\n",
            "   Rev Avg Validation Loss: 0.268\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n",
            "  Batch   900  of  1,683.    Elapsed: 0:02:03.\n",
            "  Batch 1,000  of  1,683.    Elapsed: 0:02:50.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8800\n",
            "   Normal Validation Loss: 0.652\n",
            "Rev Avg Validation Accuracy: 0.8800\n",
            "   Rev Avg Validation Loss: 0.286\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,100  of  1,683.    Elapsed: 0:00:59.\n",
            "  Batch 1,200  of  1,683.    Elapsed: 0:01:46.\n",
            "  Batch 1,300  of  1,683.    Elapsed: 0:02:33.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8740\n",
            "   Normal Validation Loss: 0.630\n",
            "Rev Avg Validation Accuracy: 0.8850\n",
            "   Rev Avg Validation Loss: 0.262\n",
            "  Validation took: 0:00:16\n",
            "  Batch 1,400  of  1,683.    Elapsed: 0:00:41.\n",
            "  Batch 1,500  of  1,683.    Elapsed: 0:01:29.\n",
            "  Batch 1,600  of  1,683.    Elapsed: 0:02:16.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:02:55\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8890\n",
            "   Normal Validation Loss: 0.561\n",
            "Rev Avg Validation Accuracy: 0.8900\n",
            "   Rev Avg Validation Loss: 0.247\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "======== Epoch 12 / 12 ========\n",
            "Training...\n",
            "  Batch   100  of  1,683.    Elapsed: 0:00:47.\n",
            "  Batch   200  of  1,683.    Elapsed: 0:01:35.\n",
            "  Batch   300  of  1,683.    Elapsed: 0:02:22.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8950\n",
            "   Normal Validation Loss: 0.562\n",
            "Rev Avg Validation Accuracy: 0.8950\n",
            "   Rev Avg Validation Loss: 0.253\n",
            "  Validation took: 0:00:16\n",
            "  Batch   400  of  1,683.    Elapsed: 0:00:46.\n",
            "  Batch   500  of  1,683.    Elapsed: 0:01:33.\n",
            "  Batch   600  of  1,683.    Elapsed: 0:02:20.\n",
            "\n",
            "Running Validation...\n",
            "Normal Validation Accuracy: 0.8860\n",
            "   Normal Validation Loss: 0.608\n",
            "Rev Avg Validation Accuracy: 0.8890\n",
            "   Rev Avg Validation Loss: 0.263\n",
            "  Validation took: 0:00:16\n",
            "  Batch   700  of  1,683.    Elapsed: 0:00:28.\n",
            "  Batch   800  of  1,683.    Elapsed: 0:01:16.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-61f530956f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m           \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw8_0RMjF1bz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22643dd5-d194-4ba3-c1e0-3e00d84caadd"
      },
      "source": [
        "print(best_val_accuracy_n) # mcl+wic_t+ausem\r\n",
        "print(best_val_accuracy_avg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.897\n",
            "0.905\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}